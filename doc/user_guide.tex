% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
%     http://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

\documentclass{article}
\def\version{2.0}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{enumitem}
\usepackage{datetime}
\usepackage{hyperref} \hypersetup{colorlinks,
  citecolor=blue,filecolor=blue,linkcolor=blue,urlcolor=black,pdftex}

\newcommand{\D}{\,\textrm{d}}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\abs}{abs}
\DeclareMathOperator{\asin}{asin}
\DeclareMathOperator{\acos}{acos}
\DeclareMathOperator{\atan}{atan}
\DeclareMathOperator{\asinh}{asinh}
\DeclareMathOperator{\acosh}{acosh}
\DeclareMathOperator{\atanh}{atanh}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\Li2}{Li_2}

\title{User guide for GADfit version \version}
\date{\monthname {} \the\year}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

GADfit is an implementation of \textbf{g}lobal nonlinear curve fitting based on \textbf{a}utomatic \textbf{d}ifferentiation (AD). Global fitting refers to fitting many data sets simultaneously with some parameters shared among the data sets but not necessarily finding the global minimum in parameter space. The optimization procedure is based on a modified Levenberg-Marquardt algorithm which is well-suited for difficult large scale problems with a strong interdependence of parameters. The Jacobian and other quantities requiring differentiation are evaluated using AD instead of finite differences which ensures that the derivatives are always calculated with the same precision as function evaluation. The cost of computing the derivatives is \textit{independent} of the number of fitting parameters and is a small constant of function evaluation resulting in very efficient code. The method can be used for fitting functions of high complexity which, in the present implementation, include nonlinear combinations of elementary and special functions, single or double integrals, and any control flow statements allowed by the programming language.

GADfit is currently hosted by GitHub.

\subsection{Contributors}

Go to \url{https://github.com/raullaasner/gadfit/graphs/contributors} to see a list of contributors.

\subsection{Terms of use}

Licensed under the Apache License, Version 2.0 (the ``License''); you may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://www.apache.org/licenses/LICENSE-2.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ``AS IS'' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

\section{Method}

GADfit is based on the Levenberg-Marquardt (LM) algorithm \cite{marquardt63}, which is a standard technique for solving nonlinear least squares problems. A least squares problem refers to minimizing the sum of the squares of the differences between the data and a curve constructed from a parameterized function. If the model curve (the fitting function) has a nonlinear dependence on the fitting parameters, it is a nonlinear least squares problem.

Given a set of $n$ data points $(x_i,y_i)$, where $x_i$ are the independent variables, and a model function $f(x, \beta_1, \ldots, \beta_k)$, which depends on $x$ and $k$ additional parameters (the fitting parameters), the sum of squares is defined as
\begin{equation}
  \label{eq:chi2}
  \chi^2 = \sum_{i=1}^n \left[ \frac{y_i - f(x_i, \bm\beta)}{\sigma_i}
  \right]^2,
\end{equation}
where $\sigma_i$ are the data point uncertainties (standard deviations). As with all nonlinear optimization algorithms, Levenberg-Marquardt is an iterative procedure. One starts with an initial guess for the parameter vector $\bm\beta$ and finds an increment $\bm\delta$ such that $\bm\beta \rightarrow \bm\beta + \bm\delta$ leads to the lowering of $\chi^2$. This is repeated until some convergence criterion is satisfied. Following the original LM method, the increment vector (the step) is given by
\begin{equation}
  \label{eq:LM}
  \left[\bm J^T \bm J + \lambda\diag \left(\bm J^T \bm J\right)
  \right] \bm\delta = \bm J^T \bm R,
\end{equation}
where\footnote{Usually the uncertainties $\sigma_i$ are not included in the
  definition.}
\begin{equation}
  \label{eq:Jacobian}
  J_{ij} = \frac{1}{\sigma_i} \frac{\partial f(x_i, \bm \beta)}
  {\partial \beta_j}
\end{equation}
defines the Jacobian,
\begin{equation}
  R_i = \frac{y_i - f(x_i,\bm \beta)}{\sigma_i}
\end{equation}
defines the residual vector, and where the damping parameter $\lambda$ allows the nature of the algorithm to adaptively change between the steepest descent and the Gauss-Newton method. If $\chi^2$ is far from its minimum, $\lambda$ should have a large value, bringing the algorithm closer to steepest descent. On the other hand, close to the minimum, Gauss-Newton is a better choice. A simple strategy for updating $\lambda$ would be as follows: if for the current iteration $\chi^2$ is smaller than for the previous iteration, accept the step and decrease $\lambda$ by a factor of 10. Otherwise increase $\lambda$, solve Eq.~\eqref{eq:LM}, and recalculate $\chi^2$ with the new step. If $\chi^2$ still increases, stop the procedure, otherwise proceed with the next iteration. If the procedure stops after the first iteration, start again with a larger initial value for $\lambda$. A sufficiently large $\lambda$ always exists that leads to a decrease of $\chi^2$ \cite{marquardt63}.

\subsection{\label{sec:mod_lm}Modified Levenberg-Marquardt}

What was described above is the classical LM algorithm, which is close to the default regime of GADfit. In this section, we discuss several interesting modifications to the algorithm (see \cite{transtrum10, transtrum11, transtrum12} and references therein), which aim to improve the convergence speed and the sensitivity to the initial guess.

\paragraph{The damping matrix.} Originally, the rationale for choosing the damping matrix as $\bm D^T\bm D \equiv \diag(\bm J^T\bm J)$ in Eq.~\eqref{eq:LM} was to make the algorithm invariant to the rescaling of parameters. This is important when the parameters have very different magnitudes and the algorithm has to follow a narrow canyon in the parameter space in search of the minimum of $\chi^2$. However, this poses a problem when the algorithm is in a region of space where the model is insensitive to the parameters. When this happens, the diagonal entries of both $\bm J^T\bm J$ and the damping matrix $\bm D^T\bm D$ become small for some parameters, which leads to uncontrollably large steps. There is a chance that some parameters are incorrectly pushed to infinite values, a phenomenon known as parameter evaporation. A compromise for both retaining the invariance to rescaling and guarding against parameter evaporation is to set the diagonal entries of $\bm D^T\bm D$ to the largest diagonal entries of $\bm J^T\bm J$ yet encountered. This approach is efficient provided that the initial guess does not lie on a plateau of the $\chi^2$ surface in the parameter space. If this condition is not satisfied then, as a simple and robust solution, the user can specify the minimum values for the diagonal of $\bm D^T\bm D$.

\paragraph{Optimal path towards the minimum.} Following a geometric interpretation of nonlinear optimization, it has been suggested that a geodesic in the parameter space is the most natural path for the algorithm to follow. When the algorithm is navigating a narrow curved canyon then, instead of taking simple steps along the gradient of $\chi^2$, it would be more efficient to take the curvature of the canyon into account and move along parabolic trajectories. This is especially beneficial for functions like the Rosenbrock function. To second order, the step is then given by
\begin{equation}
  \label{eq:delta12}
  \begin{split}
    \bm\delta =& \bm\delta_1 + \bm\delta_2, \\
    \bm\delta_1 =& \left[ \bm J^T \bm J + \lambda\bm D^T\bm D
    \right]^{-1} \bm J^T \bm R, \\
    \bm\delta_2 =& -\frac{1}{2} \left[ \bm J^T \bm J + \lambda\bm
      D^T\bm D\right]^{-1} \bm J^T \bm\Omega,
  \end{split}
\end{equation}
where (Einstein summation implied)
\begin{equation}
  \label{eq:omega}
  \Omega_i = \frac{\partial^2
    f(x_i,\beta)}{\partial\beta_j \partial\beta_k} \delta_1^j
  \delta_1^k
\end{equation}
is the second directional derivative along $\bm\delta_1$. From geometric considerations, $\bm\delta_1$ and $\bm\delta_2$ are also called the velocity and the accelerations terms. The acceleration term either magnifies or shrinks the velocity term, or tilts it in a different direction. We do not delve here into the theoretical foundations leading to Eq.~\eqref{eq:delta12} and its comparison to alternative ways of including second order corrections. The interested reader is referred to \cite{transtrum10, transtrum11, transtrum12} for an in-depth discussion. We only make a few additional remarks for practical calculations.

Geodesic acceleration is most useful in a narrow canyon. When the algorithm has not yet found a canyon and is scanning a large plateau, which is where the initial guess is likely to be, then damping is small and $\bm\delta_1$ is large. This causes $\bm\delta_2$ to be even larger and pointing in the opposite direction, which may steer the algorithm in the wrong direction. To avoid this, we require the effect of the second order term to always be smaller than the first order term,
\begin{equation}
  \label{eq:acc_alpha}
  \frac{\sqrt{\bm\delta_2\bm D\bm\delta_2}}{\sqrt{\bm\delta_1\bm
      D\bm\delta_1}} < \alpha, \quad 0 \le \alpha < 1,
\end{equation}
where $\bm D$ has been included to ensure scale invariance.

Whether including the acceleration term pays off depends on the model and is ultimately for the user to test. With automatic differentiation the second directional derivative has about the same cost as the Jacobian, which is 3--4 times function evaluation.

\paragraph{Uphill steps.} Accepting only downhill steps is the safest strategy, especially when one is far from the minimum. However, if the algorithm has to follow a narrow canyon, only very short steps would be acceptable, making the search costly. It might then be beneficial to conditionally also allow uphill steps. This can be thought of as analogous to the trajectory of a bobsled racer. A useful criterion for allowing an uphill step is
\begin{equation}
  \label{eq:uphill}
  \begin{split}
    & \beta_i = \cos(\bm\delta_{1i},\bm\delta_{1i-1}), \\
    & (1-\beta_i)^b\chi_i^2 < \min(\chi_1^2,\ldots,\chi_{i-1}^2),
  \end{split}
\end{equation}
where $\beta_i$ is the cosine of the angle between the proposed step of the current iteration and that of the previous iteration. Only the velocity and not the acceleration term is considered here. According to Eq.~\eqref{eq:uphill}, uphill steps are allowed only for acute angles, and the probability of acceptance increases with the alignment of $\bm\delta_{1i}$ and $\bm\delta_{1i-1}$. The RHS of the inequality test usually refers to the sum of squares of the previous iteration, but we use the $\min$ function here in case any of the previous steps were uphill. Reasonable values for the exponent are 1 and 2, with $b=2$ allowing greater uphill steps than $b=1$.

In addition to faster convergence, this so-called ``bold'' acceptance criterion also exhibits a smaller probability of getting stuck at the local minima, which reside on the floor of the canyon.

The drawback of this criterion is the reduced stability of the algorithm, since allowing uphill steps makes it more susceptible to parameter evaporation. It is better suited for problems where the canyon is easily found, but navigating the canyon is the main difficulty. This is in contrast to problems that start in a difficult region of the parameter space, but once heading in the right direction, the minimum is easily found.

\paragraph{Updating the damping parameter.} The simple scheme of raising and lowering $\lambda$ by a factor of 10 as the steps are accepted or rejected is often adequate, but can be improved upon. Increasing $\lambda$ by a small factor $\lambda_\uparrow$ when the step is uphill and decreasing it by a larger factor $\lambda_\downarrow$ when the step is downhill is expected to lead to a faster convergence (see Sec.~VIII.C of \cite{transtrum11}). The optimal values for the raising and lowering factors depend on the problem. One might start with, e.g., $\lambda_\uparrow=3$ and $\lambda_\downarrow=5$.

A more advanced method for updating the damping parameter makes use of the gain factor,
\begin{equation}
  \label{eq:gain_factor}
  \rho = \frac{\chi^2(\bm x, \bm\beta) - \chi^2(\bm x,
    \bm\beta+\bm\delta_1)}{\chi^2(\bm x, \bm\beta) - \left| \bm R - 2(\bm J+\lambda\bm
      D)\bm\delta_1 \right|^2} = \frac{\chi^2(\bm x, \bm\beta) -
    \chi^2(\bm x, \bm\beta+\bm\delta_1)}{2\bm\delta_1(\bm J^T\bm
    J+\lambda\bm D^T\bm D)\bm\delta_1}.
\end{equation}
The second term in the denominator is the linearly predicted value of $\chi^2$ using the new parameter vector $\bm\beta+\bm\delta_1$. A downhill step always produces a positive $\rho$ and thus the steps can be accepted or rejected based on the sign of $\rho$. If $\rho$ is close to zero, $\lambda$ is updated with $\lambda_\uparrow$ and $\lambda_\downarrow$. However, if $\rho$ is much greater than zero then the step should still be accepted but now the algorithm is stepping out of its trust-region. The next step is likely to increase $\chi^2$, which requires the lowering of $\lambda$. The oscillating behavior of $\lambda$ can be avoided by limiting the decrease of $\lambda$ so that the algorithm is unable to step out of the trust-region. Following Nielsen \cite{nielsen99}, this can be achieved by multiplying $\lambda$ by $\max[1/\lambda_\downarrow,1-(2\rho-1)^3]$ for $\rho>0$. This method tends to increase the total number of iterations, but since the steps are accepted more often, $\chi^2$ is calculated by a lesser number of times per iteration, which can overall speed up the procedure.

Another method for updating the damping parameter is due to Umrigar and Nightingale (unpublished). The idea is to vary $\lambda$ based on the history of the success of previous steps. If the current step is accepted then $\lambda$ is either decreased by a factor of $\xi$ if the change in the direction of $\bm\delta_1$ is less that $\pi/2$, left unchanged if the change is more than $\pi/2$, or increased if the step was uphill:
\begin{align}
  \label{eq:umnigh_lambda_acc}
  \lambda_i =& \lambda_{i-1}/\xi,
  & \cos(\bm\delta_{1i},\bm\delta_{1i-1}) \ge 0, \\
  \lambda_i =& \lambda_{i-1}, & \cos(\bm\delta_{1i},\bm\delta_{1i-1})
                                < 0, \\
  \lambda_i =& \lambda_{i-1}\sqrt{\xi}, & \chi_i^2 \ge \chi_{i-1}^2.
\end{align}
If the step is rejected, then $\lambda$ is always increased, but the
increase depends on the change of $\bm\delta_1$:
\begin{align}
  \label{eq:umnigh_lambda_rej}
  \lambda_i =& \lambda_{i-1}\sqrt{\xi},
  & \cos(\bm\delta_{1i},\bm\delta_{1i-1}) \ge 0, \\
  \lambda_i =& \lambda_{i-1}\xi, & \cos(\bm\delta_{1i},\bm\delta_{1i-1})
                                   < 0.
\end{align}
$\xi$ is constrained to the range 1\ldots100 and is given by
\begin{equation}
  \label{eq:umnigh_xi}
  \xi = \left( 1-|2a-1|) \right)^{-2},
\end{equation}
where $a$ is updated according to
\begin{equation}
  \label{eq:umnigh_a_update}
  a_i = a_{i-1}m + A(1-m),
\end{equation}
where
\begin{align}
  \label{eq:umnigh_A_update}
  A =& 1 && \cos(\bm\delta_{1i},\bm\delta_{1i-1}) \ge 0 \quad
            \text{and} \quad \chi_i^2 < \chi_{i-1}^2, \\
  A =& 1/2, && \cos(\bm\delta_{1i},\bm\delta_{1i-1})
               < 0 \quad \text{or} \quad \chi_i^2 \ge \chi_{i-1}^2, \\
  A =& 0 && \text{if step was rejected}.
\end{align}
The method works well in some cases but has thus far not seen extensive testing. In GADfit, the inital values for $a$ and $m$ are set to $0.5$ and $e^{-0.2}$.

\paragraph{Convergence criteria.} At the solution, the residual vector $\bm y - \bm f$ is orthogonal to the range of the Jacobian, $\bm J\bm\delta_1$. A measure of convergence can then be taken as the cosine of the angle between these two quantities,
\begin{equation}
  \label{eq:cos_phi}
  |\cos\phi| = \frac{|\bm R \bm J\bm \delta_1|}{|\bm R||\bm J\bm \delta_1|}.
\end{equation}
In addition to \eqref{eq:cos_phi}, most of the standard convergence criteria are available in GADfit. See the sections for input parameters to the fitting procedure for more details.

\paragraph{Robust cost functions} In order to damp the effect of outliers in the data, instead of minimizing the sum of squared differences [Eq.~\eqref{eq:chi2}], we can minimize the quantity
\begin{equation}
  \sum_i \rho\left[ \left(\frac{y_i - f(x_i, \bm\beta)}{\sigma_i}\right)^2 \right]
\end{equation}
instead. This leads to the following modifications of the optimization algorithm:
\begin{equation}
  \begin{split}
    & J_{ij} = \sqrt{\frac{\partial\rho(z)}{\partial z_i}} \frac{1}{\sigma_i} \frac{\partial f(x_i, \bm \beta)} {\partial \beta_j}, \\
    & R_i = \sqrt{\frac{\partial\rho(z)}{\partial z_i}} \frac{y_i - f(x_i,\bm \beta)}{\sigma_i},
  \end{split}
\end{equation}
where $\partial\rho(z)/\partial z_i$ is shorthand for
\begin{equation*}
  \left.
    \frac{\partial\rho(z)}{\partial z}
  \right|_{z = [(y_i - f(x_i,\bm \beta))/\sigma_i]^2}.
\end{equation*}
The linear case, $\rho(z) = z$, reduces to the standard least squares problem. Another option is to use the Cauchy loss function, $\rho(z) = \ln(1+z)$, which severely weakens the influence of outliers.

\subsection{Global nonlinear optimization}

When fitting many curves simultaneously with shared parameters, the data sets are stacked to form a single data set, which is then fitted with a piecewise function. The fitting function is constructed such that some parameters only have an effect on a subset of data points (local parameters) whereas others are active over the whole data range (global parameters).

As an example, take two data sets with sizes $n$ and $m$ and a fitting function $f(x,\alpha,\beta)$, which depends on two parameters $\alpha$ and $\beta$. $\alpha$ is different for the two curves whereas $\beta$ is shared. For instance, for an exponential decay process these could be different initial amplitudes but the same decay constant. The global Jacobian is then
\begin{equation}
  \label{eq:global_Jacobian}
  \bm J =
  \left( \begin{matrix}
      \frac{1}{\sigma_1} \frac{\partial f(x_1, \alpha, \beta)} {\partial\beta} & \frac{1}{\sigma_1} \frac{\partial f(x_1, \alpha, \beta)} {\partial\alpha_1} & 0 \\
      \ldots & \ldots & \ldots \\
      \frac{1}{\sigma_n} \frac{\partial f(x_n, \alpha, \beta)} {\partial\beta} & \frac{1}{\sigma_n} \frac{\partial f(x_n, \alpha, \beta)} {\partial\alpha_1} & 0 \\
      \frac{1}{\sigma_{n+1}} \frac{\partial f(x_{n+1}, \alpha, \beta)} {\partial\beta} & 0 & \frac{1}{\sigma_{n+1}} \frac{\partial f(x_{n+1}, \alpha, \beta)} {\partial\alpha_2} & \\
      \ldots & \ldots & \ldots \\
      \frac{1}{\sigma_{n+m}} \frac{\partial f(x_{n+m}, \alpha, \beta)} {\partial\beta} & 0 & \frac{1}{\sigma_{n+m}} \frac{\partial f(x_{n+m}, \alpha, \beta)} {\partial\alpha_2}
    \end{matrix} \right),
\end{equation}
where the notation is $\partial f / \partial\alpha_i \equiv \partial f / \partial\alpha |_{\alpha = \alpha_i}$. For easier implementation, the global parameters always occupy the first columns in the Jacobian. Similarly, with 4 data sets, each consisting of 100 points, and with 4 local and 2 global parameters, the Jacobian would have the dimensions $400\times18$. Sparsity of the Jacobian is not exploited. Since no conceptual change is introduced, the fitting procedure follows the same algorithm as with a single curve.

\subsection{\label{sec:ad}Automatic differentiation}

If the Jacobian \eqref{eq:global_Jacobian} was calculated using finite differences, the maximum accuracy one can hope for is half the number of significant digits corresponding to machine precision (about 7 for double precision). This is often sufficient if the fitting function is a simple combination of elementary functions such as the Gaussian or an exponential. However, problems can arise with difficult functions such as those containing a highly nonlinear combination of elementary or special functions or those requiring numerical integration. In those situations, both accuracy and the computational cost of finite differences can become problematic. Another thing to keep in mind as that with finite differences, the evaluation of $J_{ij}$ always takes two function calls and the evaluation of the whole gradient $(J_{i1}, \ldots, J_{in})$ takes $2n$ function calls.

In contrast, with automatic differentiation the derivatives are obtained with the same accuracy as function evaluation itself, while the computational cost is 3--4 times that of function evaluation \textit{irrespective} of the number of parameters \cite{griewank08}. AD exploits the fact that any mathematical function, no matter how complicated, is executed as a sequence of elementary arithmetic operations on a computer. By applying the chain rule of calculus at each step of execution, the derivative of a function can be computed automatically, with working accuracy, and using at most a small constant of the computer time required for function evaluation. It should be noted that while AD is not numerical differentiation, it is also not symbolic differentiation. The chain rule is applied directly to the results of elementary operations and the symbolic form of the full derivative is never stored in any way. Only the derivatives of the elementary functions need to be coded by hand (in AD literature, the term ``elemental function'' is more common). Afterwards, the algorithm can be applied to functions of arbitrary complexity.

In this document, we describe two flavors of AD. The forward mode of AD is well-suited for calculating the gradient of a multi-valued function depending on a single parameter (just the derivative in this case) or for calculating the directional derivative of a single-valued function depending on many parameters. The reverse mode is suitable for calculating the gradient of a single-valued function depending on many parameters. For the general case of $n$ input and $m$ output variables, a nontrivial mix of both modes would in principle be optimal.

\subsubsection{Forward mode}

The basic idea behind the forward mode is to extend all numbers to include a second component,
\begin{equation}
  \label{eq:ddouble}
  x \rightarrow \tilde x = x + \dot x d \equiv (x,\dot x),
\end{equation}
resulting in so-called dual numbers. The arithmetic on dual numbers is defined by requiring $d^2=0$. This is in contrast to ordinary complex numbers, which are also dual numbers but where the second component is defined according to $i^2=-1$. Elementary arithmetic between dual numbers can then be written as follows:
\begin{equation}
  \label{eq:dual_arith}
  \begin{split}
    \tilde x + \tilde y =& x + y + (\dot x + \dot y)d = (x+y, \dot x +
    \dot y), \\
    \tilde x\tilde y =& xy + x\dot yd + \dot xyd + \dot x\dot yd^2 =
    (xy, \dot xy + x\dot y), \\
    \frac{\tilde x}{\tilde y} =& \frac{x + \dot xd}{y + \dot yd} =
    \frac{(x + \dot xd)(y - \dot yd)}{y^2 - \dot y^2 d^2} =
    \left(\frac{x}{y}, \frac{\dot xy - x\dot y}{y^2} \right), \\
    \sin\tilde x =& \ldots = (\sin x, \dot x\cos x),
  \end{split}
\end{equation}
where the last equality follows after a series expansion. Any ordinary number can be interpreted as $\tilde x=x+0d$. It is seen from Eqs.~\eqref{eq:dual_arith} that the second component, also called the tangent, always has the form of the derivative of the operation. In fact, it can be proven that for any function $f(x)$, with a dual number as its argument,
\begin{equation}
  \label{eq:dual_general}
  f(\tilde x) = f(x) + f'(x)\dot xd = \left( f(x), f'(x)\dot x
  \right),
\end{equation}
where $f'(x)$ is the derivative of $f$ evaluated at $x$. Applying the dual number arithmetic to the composition of $f$ and $g$, we get
\begin{equation}
  \label{eq:dual_chain}
  f(g(\tilde x)) = f(g(x) + g'(x)\dot xd) = \left( f(g(x)),
    f'(g(x))g'(x)\dot x \right).
\end{equation}
With Eq.~\eqref{eq:dual_chain} we have actually rediscovered the chain rule! Repeatedly applying Eq.~\eqref{eq:dual_chain} yields the derivatives of arbitrarily complex functions.

Which derivative the dot symbol represents depends on the initial values of the tangents. It can be shown that in general the initial values define the direction of the derivative in the space of independent variables. For instance, if we have two independent variables $x$ and $y$ and require the derivative with respect to $x$, then the tangent vector is $(1,0)$ in the $xy$-plane and the variables should be seeded with $\dot x = \D x/\D x = 1$ and $\dot y = \D y/\D x = 0$. For a directional derivative $\nabla_{x,y}f(x,y) \bm v$, where $\bm v = (\alpha, \beta)$, the seeds would be $\dot x = \alpha$ and $\dot y = \beta$.

As a simple example, we shall evaluate $\frac{\partial}{\partial x} f(x, y)$ for $f(x, y) = x y + \sin (xy)$ at $(x_0,y_0)$. Applying the rules of dual numbers, the computational graph for this procedure, in terms of the intermediate results $\tilde v_i$, is
\begin{equation}
  \label{eq:forward_example}
  \begin{split}
    \tilde v_1 =& (x_0,\dot x) = (x_0,1), \\
    \tilde v_2 =& (y_0,\dot y) = (y_0,0), \\
    \tilde v_3 =& \tilde v_1\tilde v_2 = (v_1v_2, v_1 \dot v_2 +
    \dot v_1 v_2) = (x_0y_0, y_0), \\
    \tilde v_4 =& \sin \tilde v_3 = (\sin v_3, \dot v_3 \cos v_3) =
    (\sin x_0, y_0 \cos (x_0y_0)), \\
    \tilde v_5 =& \tilde v_3 + \tilde v_4 = (x_0y_0 + \sin x_0y_0, y_0
    + y_0 \cos (x_0y_0)).
  \end{split}
\end{equation}
For such a simple case, it is easy to analytically check that the tangent of $\tilde v_5$ is indeed the correct result. Note that AD is as economical as the function evaluation in terms of reusing intermediate results --- the derivative of $xy$ (the tangent of $\tilde v_3$) is computed only once even though it appears in two places. We remind that AD is not symbolic differentiation since there is no record of the symbolic form of the derivatives. The algorithm only needs to recognize the elemental operation at hand, whose derivative needs to be precoded, but otherwise the procedure is completely mechanical. At each step in the computational graph, the algorithm has forgotten how it got there. For a better illustration, here is a throwaway C++ implementation of the same calculation with $(x_0,y_0) = (1.5,0.7)$.
\begin{verbatim}
class ddouble {
public:
    double val, dot;
    ddouble(double val, double dot) : val { val }, dot { dot } {}
};
auto operator+(ddouble x, ddouble y) -> ddouble {
    return ddouble(x.val + y.val, x.dot + y.dot);
}
auto operator*(ddouble x, ddouble y) -> ddouble {
    return ddouble(x.val * y.val, x.val * y.dot + x.dot * y.val);
}
auto sin(ddouble x) -> ddouble {
    return ddouble(sin(x.val), x.dot * cos(x.val));
}
int main() {
    ddouble x(1.5, 1);
    ddouble y(0.7, 0);
    ddouble f;
    f = x * y + sin(x * y);
    std::cout << f.dot; // 1.0483
}
\end{verbatim}

If, instead of just $\frac{\partial}{\partial x}$, the whole gradient $\nabla_{x,y}f(x,y)$ is required, a second calculation needs to be performed starting with $\dot x = 0$, $\dot y = 1$. The accuracy is the same as before, but the cost is twice as much. Note that for each partial derivative, the values (first components) of the elemental operations are the same and could be reused. Due to the $O(n)$ scaling, the forward mode is generally not used for gradient calculation. Instead, it is better suited for directional derivatives, where a single sweep produces the value of $\nabla_{\bm p}f(\bm p)\bm v$, where $\bm p$ is a vector of independent variables.

In GADfit, the forward mode comes into play in only one place, which is the acceleration term \eqref{eq:delta12} which contains a second order directional derivative \eqref{eq:omega}. The generalization of the forward mode to second order is straightforward. The AD numbers now contain three components,
\begin{equation}
  \label{eq:adcomplex}
  \tilde x = x + \dot x d + \ddot x d^2,
\end{equation}
and the arithmetic is defined by requiring third and higher order terms to be zero, $d^3 = 0$. The elemental operations become somewhat more complex,
\begin{equation}
  \label{eq:2nd_order_arith}
  \begin{split}
    \tilde x + \tilde y =& x + y + (\dot x + \dot y)d + (\ddot x +
    \ddot y)d^2, \\
    \tilde x\tilde y =& xy + (\dot xy + x\dot y)d +
    (\ddot x\dot y + 2\dot x\dot y + x\ddot y)d^2, \\
    \frac{\tilde x}{\tilde y} =& \frac{x}{y} + \frac{\dot xy-x\dot
      y}{y^2}d + \frac{\ddot xy-x\ddot y-2\dot x\dot y+2\frac{x\dot
        y^2}{y}}{y^2}d^2, \\
    \sin\tilde x =& \sin x + \dot x\cos x d + (\ddot x\cos x - \dot
    x^2\sin x) d^2,
  \end{split}
\end{equation}
but otherwise the algorithm has the same structure. The complexity is actually less than appears in Eqs.~\eqref{eq:2nd_order_arith}, since once the first component of the resulting number has been calculated, it can be immediately used in the calculation of the second component. And for the third component, both of the previous terms are available. For instance, the last two of Eqs.~\eqref{eq:2nd_order_arith} can be rewritten in the form
\begin{equation}
  \label{eq:2nd_order_arith_compact}
  \begin{split}
    \tilde v =& \frac{\tilde x}{\tilde y} = v + \frac{\dot x-v\dot
      y}{y}d + \frac{\ddot x-v\ddot y-2\dot v\dot y}{y}d^2, \\
    \tilde v =& \sin\tilde x = v + \dot x\cos x d + (\ddot x\dot
    v/\dot x - \dot x^2v) d^2.
  \end{split}
\end{equation}

In order to compute the second directional derivative $\partial_i\partial_j f(x; \bm\beta) v^iv^j$, where $\bm\beta$ is the parameter vector and $\partial_i \equiv \partial/\partial\beta_i$, the parameters should be seeded with $(\beta_i, v_i, 0)$. A single sweep then produces the function value and the directional first and second derivatives.

\subsubsection{Reverse mode}

The reverse mode is generally considered superior to the forward mode because it is more efficient for calculating the gradient which is more often required than a directional derivative. The Jacobian is exactly such a quantity, where each row $(J_{i1}, \ldots, J_{in})$ is the gradient of a single-valued function $f(x_i, \bm\beta)$. At the same time, it is also harder to implement since it consists of two parts --- a forward sweep, which performs function evaluation, and a return sweep, which processes the computational graph in reverse order to yield the partial derivatives. Information about the elemental operations must be saved during the forward sweep which necessitates some form of memory management. This is in contrast to the forward mode where both the function value and the derivatives are obtained by traversing in the same direction along the computational graph and no additional memory needs to be used.

The reverse mode is best understood with an example. The sequence of elemental operations for the evaluation of $f(x,y) = xy + \sin\frac{x}{y}$ at the point $(2.3,0.8)$, which we call the forward sweep, is
\begin{subequations}
  \label{eq:forward_sweep}
  \begin{align}
    \label{eq:y1}
    v_1 &= x = 2.3000, \\
    \label{eq:y2}
    v_2 &= y = 0.8000, \\
    \label{eq:y3}
    v_3 &= v_1v_2 = 1.8400, \\
    \label{eq:y4}
    v_4 &= v_1/v_2 = 2.8750, \\
    \label{eq:y5}
    v_5 &= \sin v_4 = 0.2634, \\
    \label{eq:y6}
    v_6 &= v_3+v_5 = 2.1034,
  \end{align}
\end{subequations}
where the first two operations are just variable initialization. While the order of some of these operations can be compiler specific, it does not influence the outcome of the AD method. One might suppose that in order to compute the gradient $\nabla_{x,y}f$, it is necessary to process each of Eqs.~\eqref{eq:forward_sweep} twice, once for either parameter. However, the number of operations can be lowered by applying the chain rule backwards, i.e., by processing Eqs.~\eqref{eq:forward_sweep} in the order $v_6 \ldots v_1$ in terms of the adjoint quantities $\bar v_i \equiv \partial f/\partial v_i$. This is called the return sweep. Two trivial results are then immediately obtained from Eq.~\eqref{eq:y6}:
\begin{subequations}
  \begin{equation}
    \bar v_3 = \frac{\partial f}{\partial v_3} = \frac{\partial
      v_6}{\partial v_3} = 1, \qquad
    \bar v_5 = \frac{\partial f}{\partial v_5} = \frac{\partial
      v_6}{\partial v_5} = 1.
  \end{equation}
  Using the result for $\bar v_5$, Eq.~\eqref{eq:y5} then yields the adjoint of $v_4$,
  \begin{equation}
    \bar v_4 = \frac{\partial v_6}{\partial v_4} = \frac{\partial
      v_6}{\partial v_5}\frac{\partial v_5}{\partial v_4} = \bar v_5
    \cos v_4 = 1 \times (-0.9647) = -0.9647.
  \end{equation}
  Next, we process the two components of Eq.~\eqref{eq:y4},
  \begin{equation}
    \bar v_1 = \bar v_4 \frac{\partial v_4}{\partial v_1} = \frac{\bar
      v_4}{v_2} = -1.2059, \quad \bar v_2 = \bar v_4 \frac{\partial
      v_4}{\partial v_2} = -\frac{\bar v_4v_1}{v_2^2} = 3.4669.
  \end{equation}
  From Eq.~\eqref{eq:y3},
  \begin{equation}
    \label{eq:bar_y1_y2}
    \bar v_1 = \bar v_1 + \bar v_3 \frac{\partial v_3}{\partial v_1} =
    -0.4059, \quad \bar v_2 = \bar v_2 + \bar v_3 \frac{\partial
      v_3}{\partial v_2} = 5.7669.
  \end{equation}
\end{subequations}
The computation of $\bar v_i$ is cumulative. If some variable $v_i$ appears several times in the forward sweep then all occurrences of $v_i$ must be accounted for. This is seen in Eqs.~\eqref{eq:bar_y1_y2}, which remember the previous values of $\bar v_1$ and $\bar v_2$. With the above scheme, Eqs.~\eqref{eq:y1} and \eqref{eq:y2} would trivially yield $\bar v_1 = \bar v_1$ and $\bar v_2 = \bar v_2$ and are not processed in practice. We thus have the result
\begin{equation}
  \label{eq:Fgrad}
  \nabla_{x,y} f(x,y) = (-0.4059,5.7669).
\end{equation}
For such a simple example finite differences would of course produce the same result. However, some benefit of the reverse mode is already seen --- a single sweep produces the partial derivatives of \textit{both} variables. This property becomes especially advantageous for optimization problems with a large number of fitting parameters.

The algorithm can now be formulated as shown in Table~\ref{tab:AD}.
\begin{table}[h]
  \centering
  \caption{The reverse mode of AD. $a\mathrel{+}=b$ is a shorthand for
    $a = a + b$.}
  \begin{tabular}{c|c}
    Forward sweep & Return sweep \\ \hline
    $v_i = x_i, \quad i=1\ldots n$ & $\partial f\partial x_i = \bar
                                     v_i, \quad i=1\ldots n$ \\
    $v_i \mathrel{+}= g_i(v_{j \leadsto i}), \quad i=n+1\ldots n+m$
                  & $\bar v_i \mathrel{+}= \bar v_{j \leadsto
                    i} \partial v_{j \leadsto i}/\partial v_i$ \\
    $f = v_{n+m}$ & $\bar v_{n+m} = \partial f/\partial v_{n+m} = 1$
  \end{tabular}
  \label{tab:AD}
\end{table}
The first $n$ operations of the forward sweep initialize $n$ independent variables. The intermediate values $v_i$ are then calculated by applying the elemental operations $g_i$ to all $v_j$ of which $v_i$ directly depends on (denoted with $\leadsto$). The last one of the $m$ intermediate values is the function value, $f = v_{n+m}$. Information about each elemental operation during the forward sweep must be saved as it is required during the return sweep. The return sweep moves in the opposite direction, starting with $\partial f/\partial v_{n+m} = \partial f/\partial f = 1$ and followed by the calculation of $\bar v_i$ as illustrated in the above example. The partial derivatives are given by the first $n$ values of the adjoints array $\bm{\bar v}$.

\subsubsection{Implementation}

GADfit comes in two implementations --- in C++ and in Fortran, both based on operator overloading. The basic variable is the ``AD variable'',
\begin{verbatim}
// C++
class AdVar
{
public:
   double val, d, dd;
   int idx;
   ...
};

! Fortran
type advar
   real(kp) :: val, d = 0.0_kp, dd = 0.0_kp
   integer :: index = 0
 contains
   ...
end type advar
\end{verbatim}
This variable type is used for both forward and reverse modes. Each instance has fields for the value and first and second derivatives, and an index field. The former three are double or quadruple (Fortran only) precision floating point numbers. Index is zero for ``passive'' variables, for which differentiation is not required, and is nonzero otherwise. Which mode is currently in use is determined by the sign of the index variable (\texttt{idx}  or \texttt{index}). Whereas the \texttt{d} and \texttt{dd} fields are part of the forward mode only, the index field is required for both modes. For the forward mode, the reason to use the index is to immediately identify whether the variable is passive or active and thus many calculations involving \texttt{d=0.0} and \texttt{dd=0.0} can be avoided. In the forward mode, it only matters whether the index is zero or nonzero, whereas in the reverse mode, each new active variable is given a unique index.

The reverse mode is complicated by the fact that during the forward sweep each elemental operation\footnote{Not to be confused with the Fortran ``elemental'' keyword. In fact, these are all impure non-elemental functions.} must be recorded by saving the indices of the participating variables and of the operation code into an array called the execution trace. The intermediate (forward) values are saved into a separate array. As an example, if the variables $a$ and $b$ had indices 4 and 5, and the operation code for addition were 23, then for the operation $c = a + b$ that section of the trace would read $(\ldots, 4, 5, 6, 23, \ldots)$, where 6 is the index of the new variable $c$. For unary operations, there would be 3 entries in the trace, $(\ldots, \textmd{in}, \textmd{out}, \textmd{opcode}, \ldots)$, for ternary operations 5 entries and so on. At the beginning of each return sweep, the adjoints array is reinitialized to zero with the last element set equal to 1. The execution trace is then processed starting with the last element, and the adjoints are calculated according to the AD algorithm, first by identifying the relevant operation (\texttt{switch(op\_id)} or \texttt{case select(op\_id)}). To summarize, the main AD work variables in the reverse mode, along with their internal names, are
\begin{itemize}
\item the intermediate values (\verb+reverse_work::forwards+ or \verb+forward_values+),
\item the adjoints (\verb+reverse_work::adjoints+ or \verb+adjoints+),
\item the execution trace (\verb+reverse_work::trace+ or \verb+trace+),
\item the constants (\verb+reverse_work::constants+ or \verb+ad_constants+).
\end{itemize}
No work arrays are required for the forward mode. The current list of elemental operations includes
\begin{itemize}
% basic
\item $-x_1$, \quad $x_1\pm x_2$, \quad $x_1x_2$, \quad
  $\frac{x_1}{x_2}$, \quad $\exp(x_1)$, \quad $\sqrt x_1$, \quad $x_1^{x_2}$, \quad $|x_1|$, \quad
  $\ln x_1$,
% trigonometric
\item $\sin x_1$, \quad $\cos x_1$, \quad $\tan x_1$, \quad
  $\sinh x_1$, \quad $\tanh x_1$, \quad $\asin x_1$, \quad
  $\acos x_1$, \\ $\atan x_1$, \quad $\asinh x_1$, \quad $\acosh x_1$,
  \quad $\atanh x_1$,
% special
\item $\erf(x_1) = \frac{2}{\sqrt{\pi}} \int_0^{x_1} e^{-t^2} \D t$,
  \quad $\Li2(x_1) = -\int_0^{x_1} \frac{\ln(1-u)}{u} \D u$,
% integrals
\item
  $\int_{a(x_1, \ldots, x_n)}^{b(x_1, \ldots, x_n)} f(x_1, \ldots,
  x_n; t) \D t$,
\end{itemize}
where $x_i$ is either an AD variable, a real number of single/double/quadruple precision, or an integer. Integrals can be nested up to two layers, i.e., only single and double integrals are allowed. New elemental operations can be added to the source code without too much effort. Alternatively, any requests/suggestions are most welcome at \url{https://github.com/raullaasner/gadfit/issues}. Unless the proposed elemental operation depends on a nonstandard external library, we will make an effort to include it in the next release.

\subsubsection{AD with numerical integration}

GADfit supports fitting functions containing bounded, semi-infinite, and infinite integrals. Numerical integration is performed using the adaptive Gauss-Kronrod algorithm. Default is the 15-point rule; also available are 21, 31, 41, 51, and 61 point rules. The integration interval is divided into subintervals, and on each iteration the subinterval with the largest error is processed. This is similar to what \texttt{QUADPACK} does except without the epsilon algorithm. During this procedure, AD is temporarily switched off by making all variables passive. When the integration procedure has converged with the desired accuracy, AD is switched on and the integral is computed once more with an optimal set of subintervals. Note that the optimal set of subintervals satisfies the error tolerances of the original integral, not necessarily the integral over the derivative of the integrand. One might suppose then that the procedure needs to be performed separately for each parameter, presumably using the forward mode of AD, yielding a unique set of subintervals for each parameter. Strictly speaking though, the derivative is required for the approximation
\begin{equation}
  \label{eq:quadrature}
  F(\bm p) = \int_A^B f(\bm p,x) \D x \approx \sum_j \frac{b_j-a_j}{2}\sum_i
  w_{i,j} f \left( \bm p, \frac{b_j-a_j}{2}\xi_{i,j} + \frac{a_j+b_j}{2}
  \right),
\end{equation}
where $a_j$ and $b_j$ are the bounds of each subinterval, $w_{i,j}$ are the Gauss-Kronrod weights, and $\xi_{i,j}$ are the roots of the Legendre polynomials. Here, for brevity, $\bm p$ includes both the fitting parameters and optionally the independent variables [$x_i$ in Eq.~\eqref{eq:chi2}]. Since it is Eq.~\eqref{eq:quadrature} that is the fitting function (or part of it) and not the exact integral, differentiation should be performed using the same approximate form instead of finding a more accurate approximation for the integral over the derivative of $f$ by using a different set of subintervals.

The dependence on one or more fitting parameters can be in the integrand and/or one or both bounds. The general formula for the gradient is
\begin{multline}
  \nabla_{\bm p} \int_{a(\bm p)}^{b(\bm p)} f(\bm p, x) \D x \\
  = f(\bm p, b(\bm p)) \nabla_{\bm p} b(\bm p) - f(\bm p, a(\bm p))
  \nabla_{\bm p} a(\bm p) + \int_{a(\bm p)}^{b(\bm p)} \nabla_{\bm p}
  f(\bm p, x) \D x,
\end{multline}
where $\bm p$ is the fitting parameter vector.

The general formula for the second derivative, required for the acceleration term, is
\begin{multline}
  \nabla_{\bm p}^2 \int_{a(\bm p)}^{b(\bm p)} f(\bm p,x) \D x = f(\bm
  p,b(\bm p)) \nabla_{\bm p}^2 b(\bm p) - f(\bm p,a(\bm p))
  \nabla_{\bm p}^2 a(\bm p) \\
  + \left[ \nabla_{\bm p} f(\bm p,b(\bm p)) + \left. \nabla_{\bm\chi}
      f(\bm\chi,b(\bm p)) \right|_{\bm\chi=\bm p} \right] \nabla_{\bm
    p} b(\bm p) \\
  - \left[ \nabla_{\bm p} f(\bm p,a(\bm p)) + \left. \nabla_{\bm\chi}
      f(\bm\chi,a(\bm p)) \right|_{\bm\chi=\bm p} \right] \nabla_{\bm
    p} a(\bm p) + \int_{a(\bm p)}^{b(\bm p)} \nabla_{\bm p}^2 f(\bm p,x)
  \D x,
\end{multline}
The above scheme could be generalized to any order, but it is probably not worth going deeper than double integrals since each integration level greatly reduces accuracy. Even nested double integrals should be avoided if possible.

We also support direct double integrals with the 2D quadrature given as
\begin{multline}
  F(\bm p) = \int_{y_1(\bm p)}^{y_2(\bm p)}\int_{x_1(\bm p)}^{x_2(\bm p)} f(\bm p, x, y) \D x \D y \\
  \approx \sum_j \frac{x_{2j}-x_{1j}}{2}\frac{y_{2j}-y_{1j}}{2} \sum_i w_{i,j}^{2D} \\
  \times f \left( \bm p, \frac{x_{2j}-x_{1j}}{2}\xi_{i,j} + \frac{x_{2j}+x_{1j}}{2}, \frac{y_{2j}-y_{1j}}{2}\xi_{i,j} + \frac{y_{2j}+y_{1j}}{2} \right),
\end{multline}
where $w_{i,j}^{2D}$ are products of the 1D weights. The general formula for the gradient is
\begin{equation}
  \begin{split}
    & \bm\nabla_{\bm p} \int_{y_1(\bm p)}^{y_2(\bm p)}\int_{x_1(\bm p)}^{x_2(\bm p)} f(\bm p, x, y) \,\mathrm{d}x\,\mathrm{d}y \\
    =& \left[ \bm\nabla_{\bm p} y_2(\bm p) \right] \int_{x_1(\bm p)}^{x_2(\bm p)} f(\bm p, x, y_2(\bm p)) \,\mathrm{d}x -
    \left[ \bm\nabla_{\bm p} y_1(\bm p) \right] \int_{x_1(\bm p)}^{x_2(\bm p)} f(\bm p, x, y_1(\bm p)) \,\mathrm{d}x \\
    &+ \left[ \bm\nabla_{\bm p} x_2(\bm p) \right] \int_{y_1(\bm p)}^{y_2(\bm p)} f(\bm p, x_2(\bm p), y) \,\mathrm{d}y -
    \left[ \bm\nabla_{\bm p} x_1(\bm p) \right] \int_{y_1(\bm p)}^{y_2(\bm p)} f(\bm p, x_1(\bm p), y) \,\mathrm{d}y \\
    &+ \int_{y_1(\bm p)}^{y_2(\bm p)} \int_{x_1(\bm p)}^{x_2(\bm p)} \bm\nabla_{\bm p} f(\bm p, x, y) \,\mathrm{d}x\,\mathrm{d}y.
  \end{split}
\end{equation}
The general formula for the second derivative is
\begin{equation}
  \begin{split}
    \bm\nabla_{\bm p}^2 \int_{y_1(\bm p)}^{y_2(\bm p)}\int_{x_1(\bm p)}^{x_2(\bm p)} f(\bm p, x, y) \,\mathrm{d}x\,\mathrm{d}y =& \xi_{Y2} - \xi_{Y1} + \xi_{X2} - \xi_{X1} \\
    & + \int_{y_1(\bm p)}^{y_2(\bm p)} \int_{x_1(\bm p)}^{x_2(\bm p)} \bm\nabla_{\bm p}^2 f(\bm p, x, y) \,\mathrm{d}x\,\mathrm{d}y,
  \end{split}
\end{equation}
where
\begin{equation}
  \begin{split}
    \xi_{Y1} =& \left[ \bm\nabla_{\bm p}^2 y_1(\bm p) \right] \int_{x_1(\bm p)}^{x_2(\bm p)} f(\bm p, x, y_1(\bm p)) \,\mathrm{d}x + \left[ \bm\nabla_{\bm p} y_1(\bm p) \right] \\
    & \times \bigg[ \left[ \bm\nabla_{\bm p} x_2(\bm p) \right] f(\bm p, x_2(\bm p), y_1(\bm p)) - \left[ \bm\nabla_{\bm p} x_1(\bm p) \right] f(\bm p, x_1(\bm p), y_1(\bm p)) \\
  &+ \int_{x_1(\bm p)}^{x_2(\bm p)} \bm\nabla_{\bm\chi} f(\bm\chi, x, y_1(\bm p))|_{\bm\chi = \bm p} \,\mathrm{d}x + \int_{x_1(\bm p)}^{x_2(\bm p)} \bm\nabla_{\bm p} f(\bm p, x, y_1(\bm p)) \,\mathrm{d}x \bigg], \\
    \xi_{Y2} =& \left[ \bm\nabla_{\bm p}^2 y_2(\bm p) \right] \int_{x_1(\bm p)}^{x_2(\bm p)} f(\bm p, x, y_2(\bm p)) \,\mathrm{d}x + \left[ \bm\nabla_{\bm p} y_2(\bm p) \right] \\
    & \times \bigg[ \left[ \bm\nabla_{\bm p} x_2(\bm p) \right] f(\bm p, x_2(\bm p), y_2(\bm p)) - \left[ \bm\nabla_{\bm p} x_1(\bm p) \right] f(\bm p, x_1(\bm p), y_2(\bm p)) \\
    &+ \int_{x_1(\bm p)}^{x_2(\bm p)} \bm\nabla_{\bm\chi} f(\bm\chi, x, y_2(\bm p))|_{\bm\chi = \bm p} \,\mathrm{d}x + \int_{x_1(\bm p)}^{x_2(\bm p)} \bm\nabla_{\bm p} f(\bm p, x, y_2(\bm p)) \,\mathrm{d}x \bigg], \\
    \xi_{X1} =& \left[ \bm\nabla_{\bm p}^2 x_1(\bm p) \right] \int_{y_1(\bm p)}^{y_2(\bm p)} f(\bm p, x_1(\bm p), y) \,\mathrm{d}y + \left[ \bm\nabla_{\bm p} x_1(\bm p) \right] \\
    & \times \bigg[ \left[ \bm\nabla_{\bm p} y_2(\bm p) \right] f(\bm p, x_1(\bm p), y_2(\bm p)) - \left[ \bm\nabla_{\bm p} y_1(\bm p) \right] f(\bm p, x_1(\bm p), y_1(\bm p)) \\
    &+ \int_{y_1(\bm p)}^{y_2(\bm p)} \bm\nabla_{\bm\chi} f(\bm\chi, x_1(\bm p), y)|_{\bm\chi = \bm p} \,\mathrm{d}y + \int_{y_1(\bm p)}^{y_2(\bm p)} \bm\nabla_{\bm p} f(\bm p, x_1(\bm p), y) \,\mathrm{d}y \bigg], \\
    \xi_{X2} =& \left[ \bm\nabla_{\bm p}^2 x_2(\bm p) \right] \int_{y_1(\bm p)}^{y_2(\bm p)} f(\bm p, x_2(\bm p), y) \,\mathrm{d}y + \left[ \bm\nabla_{\bm p} x_2(\bm p) \right] \\
    & \times \bigg[ \left[ \bm\nabla_{\bm p} y_2(\bm p) \right] f(\bm p, x_2(\bm p), y_2(\bm p)) - \left[ \bm\nabla_{\bm p} y_1(\bm p) \right] f(\bm p, x_2(\bm p), y_1(\bm p)) \\
    &+ \int_{y_1(\bm p)}^{y_2(\bm p)} \bm\nabla_{\bm\chi} f(\bm\chi, x_2(\bm p), y)|_{\bm\chi = \bm p} \,\mathrm{d}y + \int_{y_1(\bm p)}^{y_2(\bm p)} \bm\nabla_{\bm p} f(\bm p, x_2(\bm p), y) \,\mathrm{d}y \bigg].
  \end{split}
\end{equation}

\section{Building and linking}

GADfit is a library, i.e., not a standalone program, written in \textit{two} different compiled languages. It started out as a Fortran implementation and was later duplicated in C++. The Fortran development has largely stopped and only the C++ implementation sees progress these days. What this means is that the user is expected to have at least basic knowledge of C++ or Fortran. Specifically, the fitting functions and the driver code would need to be written in C++/Fortran. The structure of the code is such that it is currently not possible to easily interface it with other languages. The C++/Fortran requirement might seem inconvenient for the user, but it allows to make full use of a powerful programming language for writing functions of arbitrary complexity, which is the primary benefit of using AD.

\subsection{Building process}

GADfit is dependent on the CMake build system generator. CMake is a set of tools for configuring, building, and testing software. It is released under the New BSD License and can be installed by issuing
\begin{verbatim}
  sudo apt-get install cmake
\end{verbatim}
(Here and elsewhere, we demonstrate how to obtain the relevant packages on Ubuntu; advanced users can skip this section.) For building from source visit \url{http://cmake.org}. GADfit is developed with the requirement that it should work with the GNU compilers. In principle, any C++ 20 or Fortran 2008 conforming compiler should also work.

The overall build process is similar for the C++ and Fortran versions, although they do have different prerequisites (see Secs.~\ref{sec:cxx_prerequisites} and \ref{sec:fortran_prerequisites} for details). Here we lay out instructions that apply for both.

Steps for configuring and building GADfit are the following. Untar the source code and navigate into the build directory (\verb+~build+)
\begin{verbatim}
  tar xf gadfit.tar.gz
  mkdir build && cd build
\end{verbatim}
One can also build in the source directory (\verb+~gadfit+) but it is generally a good habit to do out-of-source builds or at least create a separate build directory within the source directory.

For configuring, you can use either a graphical CMake front end, a text file containing the configuration variables, or specify the build environment from the command line (\verb+cmake <options> ...+). The two commonly used graphical front ends are the command line based \texttt{ccmake}, obtained by installing \texttt{cmake-curses-gui}, and the Qt-based \texttt{cmake-gui}, obtained by installing \\
\texttt{cmake-qt-gui}. When using \texttt{ccmake}, issue
\begin{verbatim}
  cmake ~gadfit
  ccmake .
\end{verbatim}
from the build directory. Some CMake variables and options appear, most of which should be self-explanatory. A short help text to each variable is displayed at the bottom in a status bar. Pressing 't' reveals all options. When done editing, press 'c' to reconfigure and 'g' to generate the native build script (the unix makefile on Linux). Pay attention when \texttt{ccmake} warns you that the cache variables have been reset. This will happen, e.g., when changing the compiler, and will necessitate the reconfiguring of some variables. After configuring, it is a good idea to go through all the variables once more to check that everything is correct (see Sec.~\ref{sec:cmake_configuration} for configuration variables). When finished, exit \texttt{ccmake} and issue
\begin{verbatim}
  cmake --build .
  cmake --build . --target test
  cmake --build . --target install
\end{verbatim}
The default build system generated by CMake is GNU makefiles on Linux. A different build system can be chosen by passing an argument to the CMake generator function. For instance, for using Ninja, use \texttt{-G Ninja} during the initial configuring,
\begin{verbatim}
  cmake -G Ninja ~gadfit
\end{verbatim}
In order to use \texttt{cmake-gui} instead of \texttt{ccmake}, start with
\begin{verbatim}
  cmake ~gadfit
  cmake-gui .
\end{verbatim}
The rest is analogous.

The CMake configuration variables can also be put into an external file, an example of which is \verb+initial_cache.cmake.example+ in the root directory of GADfit. It can be used as follows:
\begin{verbatim}
  cp initial_cache.cmake.example initial_cache.cmake
  # Edit initial_cache.cmake to reflect your environment
  cmake -C ~gadfit/initial_cache.cmake ~gadfit
\end{verbatim}
from the build directory, followed by the build commands. A change of the content of the configuration file has no effect after the first configuring. Instead, it would be necessary to empty \verb+~build+ before using the configuration file again or, better, use \texttt{ccmake}.

\subsubsection{\label{sec:cxx_prerequisites}C++ prerequisites}

\begin{itemize}
\item A C++ compiler. The GNU C++ compiler (g++) is usually pre-installed on a Linux distribution. If not, visit \url{gcc.gnu.org} to build and install one manually.
\item spdlog, which is a C++ logger. Install it either manually, using your distribution's package manager, or do nothing in which case the build system downloads and builds is automatically.
\end{itemize}

\subsubsection{\label{sec:fortran_prerequisites}Fortran prerequisites}

\begin{itemize}
\item A Fortran compiler
  GFortran, released under the GPL 3+ license, can be obtained with
\begin{verbatim}
  sudo apt-get install gfortran
\end{verbatim}
  or one could build from source by visiting
\begin{verbatim}
  http://gcc.gnu.org/wiki/GFortran.
\end{verbatim}
\end{itemize}

\subsubsection{\label{sec:cmake_configuration}CMake configuration variables}

See \verb+initial_cache.cmake.example+ in the root directory for a list of configuration variables. You can copy and work with that file directly. There is thus no need to list them separately here. We only mention the most imporant one here, \verb+BUILD_CXX_VERSION+, which determines whether the C++ version or the Fortran version of GADfit is built. Default is to build the C++ version.

\subsection{\label{sec:runtime}Runtime linking}

This section demonstrates how to compile and run a C++/Fortran program with calls to GADfit. Assuming we have successfully compiled GADfit as a shared library and created an input file \verb+main.cpp+/\verb+main.f90+, the following steps, if executed from the working directory, should successfully compile and execute. How to write an input file is explained in Sec.~\ref{sec:example}.

Note that you are free to use a different build system or build system generator, write your own makefile, or issue everything from the command line. However, those approaches tend to be more system specific. Here we are only focusing on how to include GADfit in CMake projects.

\subsubsection{C++}

Create a file called \verb+CMakeLists.txt+ with the following contents:
\begin{verbatim}
cmake_minimum_required(VERSION 3.13)
project(run LANGUAGES CXX)
add_executable(main main.cpp)
# If spdlog is installed to a non-standard location, specify it
# here. If installed to a standard system location or not
# installed at all, no need to do anything here.
set(spdlog_dir /usr/local/spdlog)
find_package(spdlog QUIET HINTS ${spdlog_dir})
# Directory where GADfit is installed. Can also point to the
# build directory.
set(gadfit_dir /usr/local/gadfit)
find_package(gadfit HINTS ${gadfit_dir})
target_link_libraries(main PUBLIC gadfit::gadfit)
\end{verbatim}
Edit this file to reflect your environment. Next, create a build directory, configure and compile:
\begin{verbatim}
mkdir build && cd build
cmake ..
cmake --build .
\end{verbatim}
The executable is produced in the same directory so you can run it with
\begin{verbatim}
export OMP_NUM_THREADS=8
./main
\end{verbatim}
If the C++ compiler does not have OpenMP support the export statement has no effect.

\subsubsection{Fortran}

Create a file called \verb+CMakeLists.txt+ with the following contents:
\begin{verbatim}
cmake_minimum_required(VERSION 3.13)
project(run LANGUAGES Fortran)
add_executable(main main.F90)
# Directory where GADfit is installed. Can also point to the
# build directory.
set(gadfit_dir /usr/local/gadfit)
find_package(gadfit HINTS ${gadfit_dir})
# Uncomment the MKL libraries or specify your own
target_link_libraries(main PUBLIC
  gadfit::gadfit
  # /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so
  # /opt/intel/mkl/lib/intel64/libmkl_sequential.so
  # /opt/intel/mkl/lib/intel64/libmkl_core.so
  )
\end{verbatim}
Edit this file to reflect your environment. Next, create a build directory, configure and compile:
\begin{verbatim}
mkdir build && cd build
# cmake -DCMAKE_BUILD_TYPE=Release ..
# cmake -DCMAKE_Fortran_FLAGS_RELEASE="-fcoarray=lib" ..
cmake ..
cmake --build .
\end{verbatim}
If GADfit was compiled with multi-image support, uncomment the two lines above. The executable is produced in the same directory so you can run it with
\begin{verbatim}
./main
\end{verbatim}
or
\begin{verbatim}
mpirun -np 2 main
\end{verbatim}
for the multi-image version.

For legacy reasons, we also include here a shell script for compiling and running a Fortran program with calls to GADfit.
\begin{verbatim}
#!/bin/bash

GAD=/usr/local/gadfit

# GNU
FC=mpif90
FFLAGS="-J$GAD/include -L$GAD/lib"
LIBS="$LIBS /usr/local/opencoarrays/lib/libcaf_mpi.a"
# Intel
#FC=ifort
#FFLAGS="-coarray=shared -I$GAD/include -L$GAD/lib"

# Lapack
LIBS="$LIBS -llapack"

# Atlas
#FFLAGS="$FFLAGS -L/usr/local/atlas/lib"
#LIBS="$LIBS -ltatlas"

# MKL (for Intel and GNU compilers)
#MKLROOT=/opt/intel/mkl
#FFLAGS="$FFLAGS -L$MKLROOT/lib/intel64"
#export LD_LIBRARY_PATH=$MKLROOT/lib/intel64:$LD_LIBRARY_PATH
#LIBS="$LIBS -lmkl_intel_lp64 -lmkl_sequential -lmkl_core"
#LIBS="$LIBS -lmkl_gf_lp64 -lmkl_sequential -lmkl_core -lpthread"

export LD_LIBRARY_PATH=$GAD/lib:$LD_LIBRARY_PATH
$FC $FFLAGS main.f90 -o main -lgadfit $LIBS && \
mpirun -np 4 ./main # GNU
#./main             # Intel
\end{verbatim}
This may need to be modified to reflect the user's environment. If \verb+libcaf_mpi.a+ was built automatically (see Sec.~\ref{sec:parallelism_fortran}), then its location is \\ \verb+~gadfit/fortran/tests+. If GADfit was built in serial, \verb+libcaf_mpi.a+ need not be linked.

\subsection{Notes on using GADfit in other CMake projects}

If you have built and installed GADfit and want to use it in a CMake project, only two CMake statements are necessary as demonstrated in the above examples. Assuming that GADfit has been installed at a standard system location and the target name you want to link against GADfit is \texttt{myproject}, include the lines
\begin{verbatim}
  find_package(gadfit)
  target_link_libraries(myproject PRIVATE gadfit::gadfit)
\end{verbatim}
in that project. If GADfit is not installed in a standard location, use the \texttt{PATHS} or \texttt{HINTS} argument to \verb+find_package+, e.g.,
\begin{verbatim}
  find_package(gadfit PATHS $ENV{HOME}/libs/gadfit)
\end{verbatim}
Additionally, if you need to ensure that GADfit is not older than a specific version, use
\begin{verbatim}
  find_package(gadfit 2.7 PATHS $ENV{HOME}/libs/gadfit)
\end{verbatim}
In this example, if the version of the installed copy is less than 2.7, CMake will exit with an error. Any version equal to or greater than 2.7 is fine. In order to see all the options of \verb+find_package+, run
\begin{verbatim}
  cmake --help find_package
\end{verbatim}
or consult the online documentation.

If you want to link against GADfit without installing it first, the \texttt{PATHS} argument can point directly to the GADfit build directory:
\begin{verbatim}
  find_package(gadfit PATHS $ENV{HOME}/builds/gadfit)
\end{verbatim}
This is useful for developers.

\section{Usage}

\subsection{Parallelism}

\subsubsection{C++}

The C++ implementation of GADfit is thread-safe and is parallelized using OpenMP. That means you can speed up a single instance of the solver and/or call the solver from multiple threads in an OpenMP parallel region. The number of threads working on a single instance are set by a user parameter\\
(\verb+LMsolver::settings::n_threads+) which defaults to 1. Regardless of the \verb+OMP_NUM_THREADS+ environment variable or a call to \verb+omp_set_num_threads+, the fitting procedure still runs on a single thread unless the parameter is set.

Under normal usage no OpenMP directives or functions need to be explicitly called. However, when calling some of the lower levels functions directly, such as allocating the AD tape variables, they need to be accompanied by the OpenMP parallel directive. If unsure have a look at the tests or the source code to see how certain GADfit functions must be called.

\subsubsection{\label{sec:parallelism_fortran}Fortran}

The Fortran version of GADfit is parallelized using Coarrays, a syntactic extension to the language that was included in the Fortran 2008 standard. Coarrays follow the SPMD parallelization scheme, where the main program is replicated at the start of execution. Each instance, called an image, has its own set of private variables plus so-called co-variables that are addressable by other images. Coarrays allow to parallelize programs with little effort compared to MPI or OpenMP, although it is often either MPI or OpenMP that runs under the hood.

In order to compile a Coarray program with GFortran, the compiler flag \texttt{-fcoarray=lib} must be used. For GFortran, the Coarray support is provided by the OpenCoarrays project. It can be automatically downloaded and built with \verb+cmake --build . --target example+ or \verb+cmake --build . --target test+. The Coarray communication library, \verb+libcaf_mpi.a+, is installed in \\
\verb+~gadfit/fortran/tests+. Alternatively, one may issue, e.g.,
\begin{verbatim}
  wget https://github.com/sourceryinstitute/OpenCoarrays/\
  releases/download/2.10.0/OpenCoarrays-2.10.0.tar.gz
  tar xf OpenCoarrays-2.10.0.tar.gz
  mkdir OpenCoarrays-2.10.0/build && cd OpenCoarrays-2.10.0/build
  cmake ..
  make -j # libcaf_mpi.a is in lib
\end{verbatim}
and set the \verb+OPEN_COARRAYS+ variable in the GADfit configuration to the full path of \verb+libcaf_mpi.a+. If using the Intel compiler, there is no such external dependency.

\subsection{\label{sec:example}Example input}

Suppose we have made two measurements of a decay process, which are to be fitted against
\begin{equation}
  \label{eq:exp_decay}
  I(t) = I_0e^{-t/\tau} + bgr.
\end{equation}
The decay curves correspond to the same physical phenomenon, meaning they share the same decay time $\tau$, but have been performed in slightly different experimental conditions with different initial amplitudes $I_0$ and backgrounds $bgr$. To get the best estimate for the fitting parameters, the curves should be fitted simultaneously with $\tau$ as a global fitting parameter and $I_{01}$, $I_{02}$, $bgr_1$, and $bgr_2$ as local parameters.

\subsubsection{C++}

An input file \texttt{example.cpp} that solves the nonlinear least squares problem is found in \verb+~+\texttt{gadfit/c++/tests}. Run it by invoking
\begin{verbatim}
cmake --build . --target example
\end{verbatim}

\subsubsection{Fortran}

An input file \texttt{example.F90} that solves the nonlinear least squares problem is found in \verb+~gadfit/fortran/tests+. Run it by invoking
\begin{verbatim}
cmake --build . --target example
\end{verbatim}
The data files \verb+example_data1+ and \verb+example_data2+ are required to contain at least two columns (the rest are ignored). Any line beginning with a non-number is treated as a comment. In the following, we have chosen to put the fitting function into a Fortran module, followed by the main program which contains a small set of commands to perform the fitting procedure. Users with more knowledge of Fortran may wish to do it differently. We shall first describe how to build the fitting function.

\subsection{Defining the fitting function}

\subsubsection{\label{sec:fit_function_cxx}C++}

Here is the translation of the fitting function defined by Eq.~\eqref{eq:exp_decay}:
\begin{verbatim}
static auto exponential(
  const std::vector<gadfit::AdVar>& parameters,
  const double x) -> gadfit::AdVar
{
    return parameters[0] * exp(-x / parameters[1])
           + parameters[2];
}
\end{verbatim}
It takes a parameter vector and an argument at which to evaluate the function. Definition of the function body is very simple. It reads just like a normal statement using floating point variables, except that \texttt{parameters} is a vector of AD variables. One may also use aliases for the parameter names to make it a bit more readable (see \texttt{example.cpp}).

\subsubsection{Fortran}

In the Fortran version, all user defined functions are derived from an abstract type \texttt{fitfunc} (making them \textit{derived classes} in OOP terms). Two procedures must always be defined: a constructor, which specifies the number of fitting parameters, and the function body. While the user is advised to work through a full Fortran tutorial, in the following some effort has been made to explain the peculiarities of the language.
\begin{verbatim}
module test_f
  ! This module defines the function I(x) = I0*exp(-t/tau)+bgr.
  ! In Fortran, comments begin with an "!".

  use ad ! Imports the module containing the procedures for AD.
         ! Similar to the preprocessor #include directive.
  use fitfunction ! Imports the abstract class for the fitting
                  ! function.
  use gadf_constants ! Imports the floating point precision
                     ! specifier (kp).

  implicit none ! This should be present in all modules.

  ! We have decided to call the type of the new fitting function
  ! 'exponential'. init and eval are the constructor and the
  ! function body, which must be renamed and implemented below.
  type, extends(fitfunc) :: exponential
   contains
     procedure :: init => init_exponential
     procedure :: eval => eval_exponential
  end type exponential

contains
  ! The interfaces of the following procedures, i.e., the type of
  ! the arguments and of the return value, are fixed.
  subroutine init_exponential(this)
    class(exponential), intent(out) :: this
    ! Allocate memory to the fitting parameter array.
    allocate(this%pars(3))
    ! In C++, this would be something like
    ! this->pars = new advar[3];
  end subroutine init_exponential

  type(advar) function eval_exponential(this, x) result(y)
    class(exponential), intent(in) :: this
    real(kp), intent(in) :: x
    y = this%pars(1)*exp(-x/this%pars(2)) + this%pars(3)
    ! In this context, I0 = this%pars(1), tau = this%pars(2), and
    ! bgr = this%pars(3).
  end function eval_exponential
end module test_f
\end{verbatim}
This might seem complicated at first, but have a look at \\
\verb+~gadfit/fortran/tests/function_template+, which is a stripped-down version of the above. The angle-bracketed words need to be replaced by user-defined values and the only effort lies in the description of the function body. In order to test whether the function has been correctly defined, one can use the same code as in the next section, but comment out the call to \verb+gadf_fit+. If there are no data sets, then \verb+gadf_print+ must be explicitly supplied with the arguments \verb+begin+ and \verb+end+, which define the argument range.

The above is a very simple example highlighting only some of the functionality. For more complex examples, see the tests in \verb+~gadfit/fortran/tests+.

As a comment, we have used the module approach because the user might wish in the future to put the most commonly used fitting functions into a single module that can then be conveniently included, e.g., as a library in any source file. This is just a suggestion.

\subsection{Fitting procedure}

\subsubsection{C++}

First, initialize the Levenberg-Marquardt solver,
\begin{verbatim}
gadfit::LMsolver solver { f };
\end{verbatim}
where \texttt{f} is any function with the correct signature, e.g. \texttt{exponential} from Sec.~\ref{sec:fit_function_cxx}. Next, add any number of data sets,
\begin{verbatim}
solver.addDataset(x_data_1, y_data_1);
solver.addDataset(x_data_2, y_data_2);
\end{verbatim}
If you look into \verb+lm_solver_data.h+ you'll see that the input data are of type \texttt{std::array}. Internally the data are captured by \texttt{std::span} which is a view of the data and thus avoids a copy. The data could also be of type \texttt{std::vector} or anything that is compatible with \texttt{std::span}. It is up to you to ensure that the data doesn't go out of scope before the solver is run. Then set the initial values of all fitting parameters. In this example, all starting values are set to 1.0.
\begin{verbatim}
solver.setPar(0, 1.0, true, 0);  // I0 of curve 1
solver.setPar(2, 1.0, true, 0);  // bgr of curve 1
solver.setPar(0, 1.0, true, 1);  // I0 of curve 2
solver.setPar(2, 1.0, true, 1);  // bgr of curve 2
solver.setPar(1, 1.0, true);     // tau
\end{verbatim}
Here the numeration of parameters is 0: $I_0$, 1: $\tau$, 2: bgr. This is determined by how the body of the fitting function is defined (see Sec.~\ref{sec:fit_function_cxx}). Finally, call the fitting procedure:
\begin{verbatim}
solver.fit();
\end{verbatim}

\subsubsection{Fortran}

Here are the commands necessary to run the example. For a complete overview of input variables, see Sec.~\ref{sec:input_fortran}.
\begin{verbatim}
  use test_f ! Include the above-defined fitting function
  use gadfit ! and the main library.

  implicit none

  type(exponential) :: f ! An instance of the fitting function

  ! Initialize GADfit with the fitting function and the number of
  ! data sets.
  call gadf_init(f, 2)

  ! Include both decay curves. The argument must be full or
  ! relative path to the data. TESTS_BLD is expanded by the
  ! preprocessor to ~build/fortran/tests.
  call gadf_add_dataset(TESTS_BLD//'/example_data1')
  call gadf_add_dataset(TESTS_BLD//'/example_data2')

  ! The initial guess for all fitting parameters is 1.0. The
  ! first argument denotes the data set, the second argument the
  ! parameter, third argument its value, and fourth whether the
  ! parameter is allowed to vary or is kept fixed.
  call gadf_set(1, 1, 1.0, .true.) ! I01
  call gadf_set(2, 1, 1.0, .true.) ! I02
  call gadf_set(1, 3, 1.0, .true.) ! bgr1
  call gadf_set(2, 3, 1.0, .true.) ! bgr2
  ! Global parameters don't have the data set argument.
  call gadf_set(2, 1.0, .true.)    ! tau

  ! The uncertainties of the data points determine their
  ! weighting in the fitting procedure. Here we are assuming shot
  ! noise, i.e., the error of each data point is proportional to
  ! the square root of its value. Default is no weighting.
  call gadf_set_errors(SQRT_Y)

  ! Perform the fitting procedure starting with lambda=10. If the
  ! procedure doesn't converge, we should restart with a higher
  ! value or modify any of the other arguments to gadf_fit. All
  ! the arguments are optional with reasonable default values.
  call gadf_fit(lambda=10.0)

  ! The results are saved into ~gadfit/fortran/tests
  call gadf_print(output=TESTS_BLD//'/example_results')
  call gadf_close() ! Free memory
end program
\end{verbatim}

In order to run the above code, one can manually link and compile from the command line, use a small script like in Sec.~\ref{sec:runtime}, or simply issue
\begin{verbatim}
  cmake --build . --target example
\end{verbatim}
from the build directory. If no linear algebra library has been specified, it will take a moment to build one.

The results of the calculation will be printed to the standard output and also to \verb+~build/fortran/tests/example_results_parameters+. The decay time should have the value of about 20.5. If the errors are the true experimental uncertainties, then for a good fit the reduced sum of squares should be close to 1. The results can be visualized by opening gnuplot in the directory \verb+~build/fortran/tests+ and issuing
\begin{verbatim}
  p 'example_data1' t 'data1', 'example_data2' t 'data2', \
  'example_results' u 1:2 w l t 'fit1', '' u 1:3 w l t 'fit2'
\end{verbatim}
The rest of the examples in \verb+~gadfit/fortran/tests+ can be built and run with
\begin{verbatim}
  cmake --build . --target test
\end{verbatim}
The fitting functions used in the tests are
\begin{equation*}
  \begin{split}
    f_1(x) =& f_{\text{max}}e^{-(x-x_0)^2/a^2}+bgr, \\
    f_2(x) =& \pi \int_0^x t^ae^{-bt^2} \D t, \\
    f(x) =& \int_0^\infty \D t \int_0^{x/b} \D y
    \frac{\ln[(e^y-1)(1+ab \erf t)+1]}{xy} e^{-t},
  \end{split}
\end{equation*}
where $\erf$ is the error function.

\subsection{User interface}

\subsubsection{C++}

You will mainly be interacting with the \verb+gadfit::LMsolver+ class. Here is an overview of the most important member functions and variables. Several of the functions and variables come with default values. Those are easily found in the source code (\texttt{c++/gadfit/lm\_solver.h}) and not duplicated here. The following functions and variables should be called on an \texttt{LMsolver} instance (i.e. not directly like static functions and variables).

\begin{verbatim}
gadfit::LMsolver::LMsolver(const fitSignature& function_body)
\end{verbatim}
\begin{itemize}
\item \verb+function_body+: the function body. See Sec.~\ref{sec:fit_function_cxx} for how to define one.
\end{itemize}

\begin{verbatim}
gadfit::LMsolver::addDataset(const T& x_data,
                             const T& y_data,
                             const T& errors = {})
\end{verbatim}
Add a dataset. The type \texttt{T} needs to be compatible with \texttt{std::span}, e.g. \texttt{std::vector<double>}.
\begin{itemize}
\item \verb+x_data+: array of independent variables of a data set.
\item \verb+y_data+: array of dependent variables of a data set.
\item \verb+errors+: if present, data point errors (standard deviations). Default is to set all errors equal to 1, i.e. all data points have the same weight.
\end{itemize}

\begin{verbatim}
gadfit::LMsolver::addDataset(const int n_datapoints,
                             const double* x_data,
                             const double* y_data,
                             const double* errors = nullptr)
\end{verbatim}
A low level version of \texttt{addDataset} that deals directly with pointers. Use this if pointing to a segment of an array or when the template type \texttt{T} above is incompatible with \texttt{std::span}.
\begin{itemize}
\item \verb+x_data+: shared pointer of an array of independent variables of a data set.
\item \verb+y_data+: shared pointer of an array of dependent variables of a data set.
\item \verb+errors+: if present, defines a shared pointer of data point errors. Default is to set all errors equal to 1, i.e. all data points have the same weight.
\end{itemize}

\begin{verbatim}
gadfit::LMsolver::setPar(
  const int i_par,
  const double val,
  const bool active = false,
  const int i_dataset = global_dataset_idx,
  const std::string& parameter_name = "")
\end{verbatim}
Set fitting parameters.
\begin{itemize}
\item \verb+i_par+: an index of the parameter array as defined in the fitting function.
\item \verb+val+: initial value of a fitting parameter.
\item \verb+active+: whether this is an active or a passive (constant) fitting parameter.
\item \verb+i_dataset+: if present, specifies a data set. If not present, defines a global fitting parameter.
\item \verb+parameter_name+: if present, displays the parameter name during fitting procedure. Default is to display the parameter index.
\end{itemize}

\begin{verbatim}
gadfit::LMsolver::setPar(const int i_par,
                         const double val,
                         const bool active,
                         const std::string& parameter_name)
\end{verbatim}
Call this for setting a global parameter with a name. Otherwise it is the same as the other \texttt{setPar}.

\begin{verbatim}
gadfit::LMsolver::settings
\end{verbatim}
A struct containing parameters that define runtime behavior. Find the default values in \texttt{c++/gadfit/lm\_solver.h}.
\begin{itemize}
\item \verb+iteration_limit (int)+: iteration limit.
\item \verb+lambda_down (double)+: Factor by which $\lambda$ is increased for accepted steps.
\item \verb+lambda_up (double)+: Factor by which $\lambda$ is decreased for rejected steps.
\item \verb+lambda_incs (int)+: Number of times $\lambda$ is allowed to increase consecutively without terminating the fitting procedure.
\item \verb+damp_max (bool)+: Whether to update the damping matrix with the largest entries of $J^TJ$ yet encountered. If \texttt{false}, the classical Marquardt scheme is used.
\item \verb+DTD_min (std::vector<double>)+: Minimum initial values of the diagonal entries of the damping matrix. Only relevant if \verb+damp_max == true+. Length of this array must equal the number of fitting parameters.
\item \verb+acceleration_threshold (double)+: Acceleration threshold [$\alpha$ in Eq.~\eqref{eq:acc_alpha}]. If negative, the acceleration term is not calculated.
\item \verb+loss+: Type of loss function:
  \begin{itemize}
  \item \verb+linear+: $\rho(z) = z$ where $z$ is the squared residual (default)
  \item \verb+cauchy+: $\rho(z) = \ln(1 + z)$
  \item \verb+huber+: $\rho(z) = z$ if $z \le 1$ and $\rho(z) = 2(\sqrt z - 1)$ otherwise
  \end{itemize}
\item \verb+n_threads+: Number of OpenMP threads. Default is 1.
\end{itemize}

\begin{verbatim}
gadfit::LMsolver::settings::verbosity
\end{verbatim}
Flags for controlling output. Usage follows the standard rules of bit manipulation in C++. For example, for turning on the \verb+delta1+ and \verb+delta2+ flags,
\begin{verbatim}
gadfit::LMsolver solver { f };
f.settings.verbosity = gadfit::io::delta1 | gadfit::io::delta2;
\end{verbatim}
The following flags are supported.
\begin{itemize}
\item \verb+delta1+: Show the change in parameter value after each iteration.
\item \verb+delta2+: Show the acceleration term of each parameter after each iteration. Note that the actual contribution to the update vector is $-\frac{1}{2}\bm\delta_2$. Only shown if \verb+acceleration_threshold+ is also set.
\item \verb+timings+: Show detailed timing information for all processes.
\item \verb+hide_local+: Do not show local parameters during the fitting procedure.
\item \verb+hide_global+: Do not show global parameters during the fitting procedure.
\item \verb+hide_all+: Do not show anything.
\item \verb+final_only+: Only show results after the final iteration.
\item \verb+all+: Turn on all flags.
\end{itemize}

\begin{verbatim}
gadfit::LMsolver::fit(double lambda = default_lambda)
\end{verbatim}
Performs the fitting procedure.
\begin{itemize}
\item \verb+lambda+: the adaptive damping parameter.
\end{itemize}

\begin{verbatim}
gadfit::LMsolver::getParValue(const int i_par,
                              const int i_dataset = 0)
\end{verbatim}
Get parameter \verb+i_par+ corresponding to data set \verb+i_dataset+.

\begin{verbatim}
gadfit::LMsolver::getValue(const double arg,
                           const int i_dataset = 0)
\end{verbatim}
Get the fitting function value at \texttt{arg} using parameters corresponding to data set \verb+i_dataset+.

\begin{verbatim}
gadfit::LMsolver::getJacobian()
gadfit::LMsolver::getJTJ()
gadfit::LMsolver::getDTD()
gadfit::LMsolver::getLeftSide()
gadfit::LMsolver::getRightSide()
gadfit::LMsolver::getResiduals()
gadfit::LMsolver::getInvJTJ()
\end{verbatim}
Get $\bm J$, $\bm J^T\bm J$, $\bm D^T\bm D$, left side of Eq.~\eqref{eq:LM}, right side of Eq.~\eqref{eq:LM}, the residuals $\bm y - \bm f(\bm \beta)$, or $(\bm J^T\bm J)^{-1}$, respectively.

\begin{verbatim}
gadfit::initIntegration(const int workspace_size = 1000,
                        const int n_workspaces = 1)
\end{verbatim}
Allocate memory for integration routines. This must be called before \verb+LMsolver::fit+ if the fitting function contains integrals.
\begin{itemize}
\item \verb+workspace_size+: number of Gauss-Kronrod sub-intervals.
\item \verb+n_workspaces+: number of integration levels (1 for single integrals, 2 for double integrals).
\end{itemize}

\begin{verbatim}
gadfit::freeIntegration()
\end{verbatim}
Deallocate integration workspaces.

\begin{verbatim}
spdlog::set_pattern(pattern_string)
\end{verbatim}
GADfit makes of spdlog for writing output. See the spdlog project's homepage for full documentation. Here we only describe functions that are most relevant to GADfit.
\begin{itemize}
\item \verb+pattern_string+: format for writing output. For instance, if the default output is too verbose, include something like \verb+spdlog::set_pattern("%v");+ before the first call to \verb+gadfit::LMsolver::fit+.
\end{itemize}

\begin{verbatim}
spdlog::set_level(spdlog::level)
\end{verbatim}
Set global log level
\begin{itemize}
\item \verb+spdlog::level+: logging levels.
  \begin{itemize}
  \item \verb+spdlog::level::info+: normal output.
  \item \verb+spdlog::level::debug+: more verbose output.
  \item \verb+spdlog::level::off+: turn off all output.
  \end{itemize}
\end{itemize}

\subsubsection{\label{sec:input_fortran}Fortran}

Before using GADfit, here are a some general remarks about argument processing in a Fortran.
\begin{itemize}
\item There is no automatic argument type conversion when calling a procedure. If the procedure expects a double precision number, then this is exactly what the user must supply. In C terms, if the function expects a 'double', then a 'float' will cause an error. While it can be compiler dependent, numbers such as 3.2 or 5e7 are generally interpreted as single precision. In GADfit, higher precision can be obtained by appending numbers with the ``kind parameter'' (\verb+3.2_kp+, \verb+5e7_kp+) in which case they become double or quadruple precision, depending on how GADfit was configured. With double precision, instead of \verb+_kp+ we can also use the \texttt{d} descriptor (e.g. \texttt{3.2d0}).

Internally, most procedures use at least double precision. However, for user convenience, some procedures are also available in single precision. For example, instead of
\begin{verbatim}
  call gadf_set(1, 3.2_kp)
\end{verbatim}
  we can also have
\begin{verbatim}
  call gadf_set(1, 3.2)
\end{verbatim}
  Single precision is not that important when setting initial parameter values. (The decimal point or an exponent must still be present, else the argument is interpreted as an integer.)

\item Procedure arguments can be given as a value (\verb+call gadf_fit(1.0)+) or as a name-value pair (\verb+call gadf_fit(lambda=1.0)+). When using the latter option, the arguments can be given in any order. It is also useful when calling a procedure that has many optional arguments but the user wishes to provide only some of them.
\end{itemize}
The next section contains a detailed list of all procedures that are required for normal use. The procedure arguments are given according to the format \verb+<type-specifier>::<variable>+, where the type-specifier can be \verb+real(kp)+ -- double or quad precision real number; \verb+real(real32)+ -- single precision real number; \verb+integer+ -- integer; \verb+character(*)+ -- character array (\verb+'asdf'+); \verb+logical+ -- truth value, can be either \verb+.true.+ or \verb+.false.+. If there is more than one allowed type for the argument, the choices are separated by a slash. Optional arguments have the keyword \verb+optional+. All procedures are subroutines except \verb+integrate+, which is a function.

\begin{verbatim}
gadf_init(f, num_datasets, ad_memory, sweep_size, trace_size,
          const_size, rel_error, rel_error_outer, ws_size,
          ws_size_inner, integration_rule)
\end{verbatim}
Initializes the workspace. This procedure must always be called when performing a fitting procedure.
\begin{itemize}
\item
\begin{verbatim}
class(fitfunc) :: f
\end{verbatim}
  The fitting function. \verb+class(fitfunc)+ denotes a polymorphic variable. In practice, \texttt{f} has an explicit type like \verb+type(exponential)+ in the example.
\item
\begin{verbatim}
integer, optional :: num_datasets
\end{verbatim}
  Number of data sets.

  Default: 1.
\item
\begin{verbatim}
character(*), optional :: ad_memory
\end{verbatim}
  Allocates memory for the work variables of the AD reverse mode so that \verb+forward_values+ and \verb+adjoints+ contain $x$, \verb+trace+ $4x$, and \verb+ad_constants+ $\frac{1}{2}x$ elements, where $x$ is such that the total amount of memory reserved is equal to that given by \verb+ad_memory+, whose format is ``number unit'', where ``unit'' is B, kB, MB, or GB (also acceptable are b, kb, mb, or gb). Example: \verb+ad_memory='1.25 MB'+. Overrides \verb+sweep_size+, \verb+trace_size+, and \verb+const_size+, which can be used for fine-tuning memory allocation.

  Default: none.
\item
\begin{verbatim}
integer, optional :: sweep_size
\end{verbatim}
  Size of \verb+forward_values+ and \verb+adjoints+.

  Default: 10\,000.
\item
\begin{verbatim}
integer, optional :: trace_size
\end{verbatim}
  Size of \verb+trace+.

  Default: $4\times10\,000$.
\item
\begin{verbatim}
integer, optional :: const_size
\end{verbatim}
  Size of \verb+ad_constants+.

  Default: $\frac{1}{2}\times10\,000$.
\item
\begin{verbatim}
real(kp), optional :: rel_error
\end{verbatim}
  Tolerance for the relative error of numerical integration. When dealing with double integrals, this applies to the outer integral.

  Default (single integrals): 100 $\times$ machine epsilon or\\
  Default (double integrals): 1000 $\times$ machine epsilon
\item
\begin{verbatim}
real(kp), optional :: rel_error_inner
\end{verbatim}
  Tolerance for the relative error of the inner integral.

  Default: 100 $\times$ machine epsilon.
\item
\begin{verbatim}
integer, optional :: ws_size
\end{verbatim}
  Size of the integration workspace. With double integrals this refers to the outer integral. It is assumed that there are at most this many integration subintervals.

  Default: 1000.
\item
\begin{verbatim}
integer, optional :: ws_size_inner
\end{verbatim}
  Size of the integration workspace for the inner integral when using double integrals.

  Default: 1000.
\item
\begin{verbatim}
integer, optional :: integration_rule
\end{verbatim}
  Sets the Gauss-Kronrod integration rule. Allowed values are\\
  \verb+GAUSS_KRONROD_15P+, \verb+GAUSS_KRONROD_21P+,
  \verb+GAUSS_KRONROD_31P+,\\
  \verb+GAUSS_KRONROD_41P+, \verb+GAUSS_KRONROD_51P+, and
  \verb+GAUSS_KRONROD_61P+.

  Default: \verb+GAUSS_KRONROD_15P+.
\end{itemize}

\begin{verbatim}
gadf_add_dataset(path)
\end{verbatim}
Reads a data set. This procedure must be called as many times as there are data sets, which is determined by the \verb+num_datasets+ argument to \verb+gadf_init+.
\begin{itemize}
\item
\begin{verbatim}
character(*) :: path
\end{verbatim}
  Full or relative path to the data file.
\end{itemize}

\begin{verbatim}
gadf_add_dataset(x_data, y_data, weights)
\end{verbatim}
Reads a data set. This procedure must be called as many times as there are data sets, which is determined by the \verb+num_datasets+ argument to \verb+gadf_init+.
\begin{itemize}
\item
\begin{verbatim}
real(kp) :: x_data(:)
\end{verbatim}
  Array containing the x-values (data point coordinates).
\item
\begin{verbatim}
real(kp) :: y_data(:)
\end{verbatim}
  Array containing the y-values (data point values).
\item
\begin{verbatim}
real(kp), optional :: weights(:)
\end{verbatim}
  Array containig the data point weights. Only used with the \texttt{USER} argument to \verb+gadf_set_errors+ (see below).
\end{itemize}

\begin{verbatim}
gadf_set(dataset_i, par, value, active)
\end{verbatim}
Defines the parameter as local, sets its value, and marks it either active or passive.
\begin{itemize}
\item
\begin{verbatim}
integer :: dataset_i
\end{verbatim}
  The data set index.
\item
\begin{verbatim}
integer/character(*) :: par
\end{verbatim}
  Either the parameter index or its name.
\item
\begin{verbatim}
real(real32)/real(kp) :: val
\end{verbatim}
  Parameter value in either single or higher precision.
\item
\begin{verbatim}
logical, optional :: active
\end{verbatim}
  If \verb+.true.+ the parameter is active, else passive. In the current implementation, if a parameter is local it is either active or passive for all data sets.

  Default: \verb+.false.+.
\end{itemize}

\begin{verbatim}
gadf_set(par, value, active)
\end{verbatim}
Similar to the other \verb+gadf_set+ except that this one defines either a global fitting parameter or a global constant. If only one data set is used, it doesn't matter which procedure is used.

\begin{verbatim}
gadf_set_verbosity(scope, digits, timing, memory, workloads,
                   delta1, delta2, cos_phi, grad_chi2, uphill,
                   acc, output)
\end{verbatim}
Gives the user some control over how the results are displayed. All flags can be set to \texttt{.true.}. For instance, one might wish know $|\cos\phi|$ of Eq.~\eqref{eq:cos_phi} but not used it as a convergence criterion.
\begin{itemize}
\item
\begin{verbatim}
integer, optional :: scope
\end{verbatim}
  Whether to show only local or global parameters or both during the fitting procedure. Allowed values are \verb+GLOBAL+, \verb+LOCAL+, and \verb+GLOBAL_AND_LOCAL+.

  Default: \verb+GLOBAL_AND_LOCAL+.
\item
\begin{verbatim}
integer, optional :: digits
\end{verbatim}
  How many significant digits of the fitting parameters are shown during the fitting procedure.

  Default: 7.
\item
\begin{verbatim}
logical, optional :: timing
\end{verbatim}
  Whether to show the timing summary after the fitting procedure. The cpu and wall times are measured for $\bm J$, $\chi^2$, $\bm\Omega$, and simple linear algebra operations ($\bm J^T$, $\bm J^T\bm J$, $\bm J^T(\bm y - \bm f), \ldots$). The cost of calculating $\bm J$ also includes the residual vector $\bm y - \bm f$, which is produced as part of the forward sweep. All other parts of the fitting procedure are serial. Since the serial portion of the code is negligible, all images should in principle do the same amount of work. The timing summary thus contains information about the load imbalance. It contains the following parts:
  \begin{itemize}
  \item The cpu times per call are averaged over images and over the number of calls to the procedure.
  \item The relative cost is the cpu time averaged over images and divided by the total cpu time spent in the main loop. With perfect scaling the relative costs should add up to 100\%.
  \item Detailed timing is shown for $\bm J$, $\chi^2$, and $\bm\Omega$. The reported total cpu and wall times are over all serial and parallel parts of the main loop, and do not correspond exactly to the sum of the shown quantities.
  \end{itemize}

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: memory
\end{verbatim}
  Whether to show memory usage after the fitting procedure. Only the peak usage is shown, which is not necessarily representative of the actual load.

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: workloads
\end{verbatim}
  Whether to show the workload of each image. If the loads are very uneven, it is worth considering using load balancing.

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: delta1
\end{verbatim}
  Whether to show $\bm\delta_1$, the velocity term.

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: delta2
\end{verbatim}
  Whether to show $\bm\delta_2$, the acceleration term.

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: cos_phi
\end{verbatim}
  Whether to show $|\cos\phi|$ [Eq.~\eqref{eq:cos_phi}].

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: grad_chi2
\end{verbatim}
  Whether to show $|\nabla \chi^2| = |2\bm J^T (\bm y - \bm f)|$.

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: uphill
\end{verbatim}
  Whether to show $\cos(\bm\delta_i,\bm\delta_{i-1})$
  [Eq.~\eqref{eq:uphill}].

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: acc
\end{verbatim}
  Whether to show the ratio of the contributions from the velocity and the acceleration terms [Eq.~\eqref{eq:acc_alpha}].

  Default: \verb+.false.+.
\item
\begin{verbatim}
character(*), optional :: output
\end{verbatim}
  Where to send the output. Can be a file or, in general, any I/O device. On Linux, using \verb+'/dev/null'+ suppresses all output except errors and warnings, which are sent to stderr.

  Default: stdout (standard output).
\end{itemize}

\begin{verbatim}
gadf_set_errors(e)
\end{verbatim}
Specifies the data point errors -- $\sigma_i$ in Eq.~\eqref{eq:Jacobian}. If this procedure is not called, the errors are set to 1.
\begin{itemize}
\item
\begin{verbatim}
integer, optional :: e
\end{verbatim}
  Allowed values are \verb+NONE+ -- $\sigma_i = 1$ (the default); \verb+SQRT_Y+ -- $\sigma_i = \sqrt{y_i}$ (shot noise); \verb+PROPTO_Y+ -- $\sigma_i = y_i$; \verb+INVERSE_Y+ -- $\sigma_i = 1/y_i$; \verb+USER+ -- user defined uncertainties, must be supplied as the 3rd column in the data files or as a separate array.
\end{itemize}

\begin{verbatim}
gadf_fit(lambda, lam_up, lam_down, accth, grad_chi2, cos_phi,
         rel_error, rel_error_global, chi2_rel, chi2_abs,
         lam_incs, uphill, max_iter, damp_max, nielsen, umnigh,
         load_balancing, use_ad)
\end{verbatim}
Performs the fitting procedure. The procedure stops if any of the convergence criteria is satisfied.
\begin{itemize}
\item
\begin{verbatim}
real(real32), optional :: lambda
\end{verbatim}
  The adaptive damping parameter.

  Default: 1.
\item
\begin{verbatim}
real(real32), optional :: lam_up
\end{verbatim}
  Factor by which $\lambda$ is increased for accepted steps.

  Default: 10.
\item
\begin{verbatim}
real(real32), optional :: lam_down
\end{verbatim}
  Factor by which $\lambda$ is decreased for rejected steps.

  Default: 10.
\item
\begin{verbatim}
real(real32), optional :: accth
\end{verbatim}
  Acceleration threshold [$\alpha$ in Eq.~\eqref{eq:acc_alpha}]. If zero, the acceleration term is not calculated.

  Default: 0.
\item
\begin{verbatim}
real(real32), optional :: grad_chi2
\end{verbatim}
  Tolerance for $|\nabla \chi^2| = |2\bm J^T (\bm y - \bm f)|$.

  Default: 0.
\item
\begin{verbatim}
real(real32), optional :: cos_phi
\end{verbatim}
  Tolerance for the cosine of the angle between the residual vector and the range of the Jacobian [Eq.~\eqref{eq:cos_phi}].

  Default: 0.
\item
\begin{verbatim}
real(real32), optional :: rel_error
\end{verbatim}
  Tolerance for the relative change in any fitting parameter for two consecutive iterations.

  Default: 0.
\item
\begin{verbatim}
real(real32), optional :: rel_error_global
\end{verbatim}
  Same as \verb+rel_error+ but applies only to global parameters.

  Default: 0.
\item
\begin{verbatim}
real(real32), optional :: chi2_rel
\end{verbatim}
  Tolerance for the relative change in $\chi^2$ for two consecutive iterations.

  Default: 0.
\item
\begin{verbatim}
real(real32), optional :: chi2_abs
\end{verbatim}
  Tolerance for the reduced sum of squares.

  Default: 0.
\item
\begin{verbatim}
real(kp), optional :: DTD_min(:)
\end{verbatim}
  Minimum values of the diagonal entries of the damping matrix.

  Default: 0.
\item
\begin{verbatim}
integer, optional :: lam_incs
\end{verbatim}
  Number of times $\lambda$ is allowed to increase consecutively without terminating the fitting procedure.

  Default: 2.
\item
\begin{verbatim}
integer, optional :: uphill
\end{verbatim}
  The exponent $b$ in Eq.~\eqref{eq:uphill}. If zero, uphill steps are not allowed.

  Default: 0.
\item
\begin{verbatim}
integer, optional :: max_iter
\end{verbatim}
  Iteration limit.

  Default: unlimited.
\item
\begin{verbatim}
logical, optional :: damp_max
\end{verbatim}
  Whether to update the damping matrix with the largest entries of $J^TJ$ yet encountered. If \texttt{.false.}, the classical Marquardt scheme is used.

  Default: \texttt{.true.}.
\item
\begin{verbatim}
logical, optional :: nielsen
\end{verbatim}
  Whether to update $\lambda$ with $\max[1/\lambda_\downarrow,1-(2\rho-1)^3]$. See the paragraph on updating $\lambda$ in Sec.~\ref{sec:mod_lm}. $\rho$ includes only the velocity term.

  Default: \texttt{.false.}.
\item
\begin{verbatim}
logical, optional :: umnigh
\end{verbatim}
  Whether to update $\lambda$ according to Umrigar and Nightingale.

  Default: \texttt{.false.}.
\item
\begin{verbatim}
logical, optional :: load_balancing
\end{verbatim}
  Whether to use load balancing. If \verb+.true.+, the input data is redistributed among the images in an effort to achieve equal workloads. The redistribution is based on the workloads of the previous iteration and is repeated after each iteration. Load balancing makes sense with integrals and other constructs whose cost heavily depends on the argument.

  Default: \verb+.false.+.
\item
\begin{verbatim}
logical, optional :: use_ad
\end{verbatim}
  Whether to use automatic differentiation for the computation of the Jacobian (both the velocity and acceleration terms). If false, use finite differences.

  Default: \verb+.true.+.
\end{itemize}

\begin{verbatim}
gadf_print(begin, end, points, output, grouped, logplot,
           begin_kp, end_kp)
\end{verbatim}
Prints the results to an I/O device. The results include the theoretical curve(s), fitting parameter values, and summary of resources used. If no fitting procedure has been performed, only the theoretical curve is printed.
\begin{itemize}
\item
\begin{verbatim}
real(real32), optional :: begin
\end{verbatim}
  The lower bound for the $x$-values.

  Default: taken from the input data.
\item
\begin{verbatim}
real(real32), optional :: end
\end{verbatim}
  The upper bound for the $x$-values.

  Default: taken from the input data.
\item
\begin{verbatim}
integer, optional :: points
\end{verbatim}
  Number of points printed per curve.

  Default: 200.
\item
\begin{verbatim}
character(*), optional :: output
\end{verbatim}
  Output file root name.

  Default: \verb+'out'+.
\item
\begin{verbatim}
logical, optional :: grouped
\end{verbatim}
  Whether to group all curves into a single file.

  Default: \verb+.true.+.
\item
\begin{verbatim}
logical, optional :: logplot
\end{verbatim}
  Whether to produce results suitable for a $(\log x,y)$-plot.

  Default: \verb+.false.+.
\item
\begin{verbatim}
real(kp), optional :: begin_kp
\end{verbatim}
  Same as \verb+begin+ but with double/quad precision; overrides
  \verb+begin+.

  Default: none.
\item
\begin{verbatim}
real(kp), optional :: end_kp
\end{verbatim}
  Same as \verb+end+ but with double/quad precision; overrides
  \verb+end+.

  Default: none.
\end{itemize}

\begin{verbatim}
gadf_close()
\end{verbatim}
Frees all work variables. After this, it is safe to call
\verb+gadf_init+ again.

\begin{verbatim}
integrate(f, pars, lower, upper, rel_error, abs_error)
\end{verbatim}
Performs numerical integration. Can be used as part of a fitting function. The result is of type \verb+advar+.
\begin{itemize}
\item
\begin{verbatim}
type(advar) function f(x, pars)
  real(kp), intent(in) :: x
  type(advar), intent(in out) :: pars(:)
end function f
\end{verbatim}
  The integrand.
\item
\begin{verbatim}
type(advar) :: pars(:)
\end{verbatim}
  Parameters passed to the integrand.
\item
\begin{verbatim}
type(advar)/real(kp)/integer :: lower
\end{verbatim}
  Lower integration bound. Can be an active/passive fitting parameter, a real number, \verb+-INFINITY+, or \verb+INFINITY+.
\item
\begin{verbatim}
type(advar)/real(kp)/integer :: upper
\end{verbatim}
  Upper integration bound. Same comments as with \verb+lower+.
\item
\begin{verbatim}
real(kp), optional :: relative_error
\end{verbatim}
  Tolerance for the relative error of the integral. Overrides \verb+gadf_init+.

  Default: whatever was specified with \verb+gadf_init+ or 100 (1000) $\times$ machine epsilon for the inner (outer) integral.
\item
\begin{verbatim}
real(kp), optional :: absolute_error
\end{verbatim}
  Tolerance for the absolute error.

  Default: 0.
\end{itemize}

\subsection{Internals}

\subsubsection{Fortran}

The previous sections were about the main commands that would normally be used. This section describes more advanced usage of GADfit for experimental, debugging, or other purposes. Some knowledge of Fortran is assumed.

\paragraph{Basic usage of the \texttt{fitfunc} class.} As was mentioned earlier, the user can test the fitting function by calling \verb+gadf_print+ without performing a fitting procedure. It is, however, possible to work directly with the fitting function, which inherits from the \verb+fitfunc+ class. The following code snippet prints the value of a function with the argument 2.0. It is assumed that GADfit has been compiled with double precision.
\begin{verbatim}
  type(test) :: f              ! 'test' extends fitfunc.
  real(dp) :: dummy
  !call init_integration()     ! if necessary or
  !call init_integration_dbl() ! if necessary.
  call f%init()
  call f%set(1, 1d0)           ! Has just one parameter.
  dummy = f%eval(2d0)          ! Converts AD variable to real.
  write(*,*) dummy             ! Function value at 2.0.
  call f%destroy()
  call free_integration()      ! Optional but never hurts.
\end{verbatim}
Here and elsewhere, a dummy variable must be used since a component of a function result of derived type cannot be directly referenced in Fortran.

If the only aim is to print the function value(s) at some point(s), then the code can be slightly reduced by using the procedures of Sec.~\ref{sec:input_fortran}. After calls to \verb+gadf_init+ and \verb+gadf_set+, multiple instances of the function, each with an independent set of parameters, are copied to the \verb+fitfuncs+ array, which is protected (read-only public). The above code then reduces to
\begin{verbatim}
  type(test) :: f
  real(dp) :: dummy
  call gadf_init(f)
  call gadf_set(1, 1d0)
  dummy = fitfuncs(1)%eval(2d0)
  write(*,*) dummy
  call gadf_close()
\end{verbatim}
Another reason for directly working with the \verb+fitfunc+ class is the calculation of derivatives using finite differences, which can be used for testing new AD elemental operations. In the next snippet, the derivatives are calculated with respect to parameters 1 and 3, while parameter 2 is passive.
\begin{verbatim}
  type(test) :: f
  real(dp) :: grad(2)
  call f%init()
  call f%set(1, 1.2d0)
  call f%set(2, -0.1d0)
  call f%set(3, 1d17)
  call f%grad_finite(2d0, [1,3], grad)
  write(*,*) grad
  call f%destroy()
\end{verbatim}
In order to calculate the 2nd directional derivative in the $(0.3,0.7)$ direction in the space spanned by parameters 1 and 3, the above code can be modified by inserting
\begin{verbatim}
  write(*,*) f%dir_deriv_2nd_finite(2d0, [1,3], [0.3d0,0.7d0])
\end{verbatim}
after the parameters have been initialized. See the documentation of \\
\verb+grad_finite+ and \verb+dir_deriv_2nd_finite+ in \verb+fitfunction.f90+ for a better understanding.

Both finite differences and AD are always available regardless of the value of \verb+USE_AD+ during configuration. This option only determines which one is used during the fitting procedure.

\paragraph{Basic usage of the AD forward mode.} In the forward mode, parameters are made active by modifying both their \texttt{index} and \texttt{d} fields. In the interest of compactness, here we shall set the parameter values directly instead of using the more safe \texttt{set} procedure.
\begin{verbatim}
  type(test) :: f
  type(advar) :: dummy
  call f%init()
  f%pars = [1.2d0, -0.1d0, 1d17]
  f%pars%index = [1,0,0]
  f%pars%d = [1,0,0]
  dummy = f%eval(2d0)
  write(*,*) dummy%d
  f%pars%index = [0,0,1]
  f%pars%d = [0,0,1]
  dummy = f%eval(2d0)
  write(*,*) dummy%d
  call f%destroy()
\end{verbatim}
In order to calculate the 1st and 2nd directional derivatives in the $(0.3,0.7)$ direction, the above code becomes
\begin{verbatim}
  type(test) :: f
  type(advar) :: dummy
  call f%init()
  f%pars = [1.2d0, -0.1d0, 1d17]
  f%pars%index = [1,0,1]
  f%pars%d = [0.3,0.0,0.7]
  dummy = f%eval(2d0)
  write(*,*) 'First  dir deriv:', dummy%d
  write(*,*) 'Second dir deriv:', dummy%dd
\end{verbatim}

\paragraph{Basic usage of the AD reverse mode.} The reverse mode is a bit more difficult to use manually since the user has to be more acquainted with the internal work variables of \verb+automatic_differentiation.f90+. In particular, it is necessary to set \verb+index_count+ to the number of active parameters (index of the next AD variable will be \verb+index_count++1), and to initialize the array of intermediate values. The gradient calculation with respect to parameters 1 and 3, as in the previous examples, can then be achieved as follows.
\begin{verbatim}
  type(test) :: f
  real(dp) :: dummy
  call ad_init_reverse()
  call f%init()
  f%pars = [1.2d0, -0.1d0, 1d17]
  f%pars%index = [1,0,2]
  index_count = 2
  forward_values(1:2) = [f%pars(1)%val, f%pars(3)%val]
  dummy = f%eval(2d0)
  !write(*, '(g0)') trace(:trace_count)
  call ad_grad(2)
  write(*,*) adjoints(1:2)
  call f%destroy()
  call ad_close()
\end{verbatim}
Uncommenting the line after function evaluation shows what the execution trace looks like after the forward sweep. If the function contains an integral, additional calls to \verb+init_integration+ or \verb+init_integration_dbl+ and \verb+free_integration+ are required.

\paragraph{Numerical integration.} It is also possible to use GADfit just for evaluating integrals, although if this is the only purpose, there are better tools out there. The following example shows the calculation of $\int_{0.5}^\infty e^{-x}\D x$:
\begin{verbatim}
  type(advar) :: dummy, pars(0)
  call init_integration()
  dummy = integrate(f, pars, 0.5d0, INFINITY)
  print*, dummy%val
  call free_integration()
contains
  type(advar) function f(x, pars) result(y)
    real(kp), intent(in) :: x
    type(advar), intent(in out) :: pars(:)
    y = exp(-x)
  end function f
\end{verbatim}

\paragraph{Creation of new elemental operations.} New elemental operations can be introduced in \verb+automatic_differentiation.f90+. The main steps are:
\begin{itemize}
\item A new operation code is created.
\item A module procedure is overloaded.
\item In the reverse mode, the operation body is supposed to do the following: the function body is evaluated; the newly created result variable is given a unique index; the result of evaluation is saved in \verb+forward_values+; the indices of all arguments, that of the result variable, and the operation code are saved in \verb+trace+; any constants are saved in \verb+ad_constants+; \verb+index_count+, \verb+trace_count+, and \verb+const_count+ are increased accordingly. In the forward mode, the implementation is straightforward. In either case, if the argument is a passive variable, only the function body is evaluated. For a better understanding, have a look at the implementation of any unary operation in the source code.
\item In the reverse mode, the calculation of the adjoints is implemented in the return sweep (\verb+ad_grad+).
\end{itemize}

\section{Developer notes}

\subsection{C++}

\subsubsection{Unit tests}

Unit tests are written using the Catch2 framework. Catch2 needs to be installed separately if not already present on your system. The tests can be run with
\begin{verbatim}
  cmake --build . --target test
\end{verbatim}
or you can run them manually one by one. Each test is an executable in \texttt{c++/tests} in your build directory.

\subsubsection{Code linters}

Before comitting, you should run clang-tidy on the source code. CMake has built-in support for that so all you need to do is run
\begin{verbatim}
cmake -DCMAKE_CXX_CLANG_TIDY=clang-tidy .
\end{verbatim}
in the build directory and recompile. You can keep this on but if it noticeably slows down compilation you might want to turn it off during development with
\begin{verbatim}
cmake -U CMAKE_CXX_CLANG_TIDY .
\end{verbatim}
We also use the clang-format code linter. However, since CMake does not have built-in support for clang-format it is up to the developer to find a way to run clang-format on the source code. As an example, you may run this in the directory with the C++ source files:
\begin{verbatim}
find -regextype posix-extended -regex ".*\.(cpp|h)" -exec \
sh -c 'clang-format -style=file {} | diff -u {} -' \;
\end{verbatim}
If the code is perfectly valid, there should be not output. Otherwise, you should resolve the diff. This requirement does not hold for the tests directory.

\subsubsection{Code coverage}

Code coverage can be inspected by enabling the \verb+INCLUDE_COVERAGE+ CMake variable. This only works with the GNU compiler and requires LCOV to be installed. The flags defined by \verb+COVERAGE_FLAGS+ are then appended to the compilation flags (the default ``--coverage'' should be fine). If you then recompile and run the tests, it shows you the overall coverage rate for lines and functions and produces a detailed HTML report that you can inspect. Here is an example of how to programmatically turn on the coverage flag, run the coverage, and then turn it off again:
\begin{verbatim}
cmake -DINCLUDE_COVERAGE=ON .
cmake --build . --target coverage
cmake -DINCLUDE_COVERAGE=OFF .
\end{verbatim}

\subsubsection{Coding rules}

Follow the C++20 standard. Otherwise, there are no additional coding rules because clang-tidy and clang-format are already quite exhaustive. If those pass you're good to go.

\subsection{Fortran}

\subsubsection{Coding rules}

\textit{Programming principles}

\begin{itemize}
\item \textbf{Standard-conforming code.} This project is committed to following the Fortran 2008 standard (F2008), which is a minor improvement over the 2003 standard, but a major improvement over the commonly used Fortran 90, making coding much more efficient. It is not possible to list here all the rules that the developer is expected to follow when writing modern Fortran. There is not always a unique answer anyway. Many rules are expected to be self-evident. Here we concentrate on rules specific to this project and that are not standard in general for modern Fortran. A good place to start is to examine an existing module in addition to reading this.
\item \textbf{Exception handling.} Use \verb+check_err+ after allocate and open statements. \verb+safe_close+ is optional and depends on factors such as the amount of data written. For deallocation use \verb+safe_deallocate+ and, if necessary, define a new subroutine for the \verb+safe_deallocate+ interface. For reporting errors, warnings, and comments, use the appropriate procedures from the messaging module.
\item \textbf{Compilation stage.} The code should compile without any warnings when using debug flags with GFortran and preferably also with Ifort, although the latter is not a requirement.
\item \textbf{Preprocessor.} Use preprocessor macros only for actual preprocessing and not for something that is a short-hand notation for a Fortran statement. Excessive use of macros makes it difficult for source code indexing tools to navigate through the source code.
\end{itemize}

\noindent\textit{Format}

\begin{itemize}
\item \textbf{Statement order.} A module should have the following format:
\begin{verbatim}
  module <module_name>
  <'use' statements in alphabetical order>
  implicit none
  private
  protected :: <list>
  public :: <list>
  <enumerators>
  <constants>
  <derived type definitions>
  <interfaces>
  <variables>
  contains
  <procedures>
  end module <module_name>
\end{verbatim}
  Protected variables should not be listed after the \texttt{public} statement, but declared public elsewhere. Also, the default \texttt{private} attribute is preferred, but \texttt{public} is also acceptable where it make sense.
\item \textbf{Order of arguments.} The order of arguments to a subprogram is preferably the following: the \texttt{pass} argument, \texttt{intent(in)} arguments, \texttt{intent(in out)} arguments, \texttt{intent(out)} arguments, \texttt{optional} arguments.
\item \textbf{Case.} Use lower-case everywhere except for named constants, enumerators, and preprocessor keywords. Known mathematical constants and variables can be mixed-case if more readable this way.
\item \textbf{Names.} Use self-documenting variable names. The larger the scope the more informative the name should be. Do not use the Hungarian notation. Unless there is a good reason to do otherwise, the result variable of a function should be \texttt{y} with the type specified in the \texttt{function} line.
\item \textbf{Spaces.} Use spaces where allowed (\texttt{in out}, \texttt{end do}, etc.). Use one space around most binary operators.
\item \textbf{Kind.} Do not use syntax such as \texttt{real*8} or \texttt{real(8)}. Make use of constants such as \texttt{dp} or \texttt{kp}.
\item \textbf{Line length.} Limit the line length to 80 symbols. Not only is it good for readability, but the compiler can be more informative about the line number where the error occured. For continuation use ampersand on both lines.
\item \textbf{Indentation.} 3 spaces for \texttt{do} and \texttt{if} constructs.
\end{itemize}

\subsection{Version control}

GADfit is versioned using the Git version control system and all contributors are expected to be fairly knowledgeable about Git. If unsure, work through the first three chapter of the Git book: \url{https://git-scm.com/book/en/v2} (then work through the rest of it).

Here is a summary of the rules for writing Git commit messages:
\begin{itemize}
\item Separate subject from body with a blank line.
\item Limit the subject line to 50 characters.
\item Capitalize the subject line.
\item Do not end the subject line with a period.
\item Use the imperative mood in the subject line.
\item Wrap the body at 72 characters.
\item Use the body to explain the \textit{what} and \textit{why} and not the \textit{how}.
\end{itemize}
See this blog post for a full exaplanation: \url{https://chris.beams.io/posts/git-commit}.

\section{Summary}

\noindent\textbf{Title of the program}

GADfit

\noindent\textbf{URL for the source code}

\url{https://github.com/raullaasner/gadfit}

\noindent\textbf{Programming languages}

C++, Fortran (two independent implementations)

\noindent\textbf{Distribution format}

Git, gzip, zip

\noindent\textbf{Nature of the problem}

Nonlinear least squares regression

\noindent\textbf{Method of solution}

Improved Levenberg-Marquardt with multi-curve fitting

\noindent\textbf{License}

Apache License 2.0

\noindent\textbf{Supported operating systems}

Linux

\section{Troubleshooting}

A good place to bring up any issues is \\ \url{https://github.com/raullaasner/gadfit/issues}.

\bibliographystyle{physrev}
\bibliography{refs}

\end{document}
